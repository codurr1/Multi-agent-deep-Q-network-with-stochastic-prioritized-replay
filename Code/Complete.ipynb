{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "NfY4FtkL1j9x"
      },
      "id": "NfY4FtkL1j9x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62239b8",
      "metadata": {
        "id": "f62239b8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os \n",
        "import random \n",
        "import argparse \n",
        "import pandas as pd \n",
        "import glob\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf465e9",
      "metadata": {
        "id": "ddf465e9"
      },
      "source": [
        "\n",
        "### NeuralNetwork\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4178e040",
      "metadata": {
        "scrolled": false,
        "id": "4178e040"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Lambda, Input, Concatenate, Activation\n",
        "from keras.optimizers import adam_v2\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "    \n",
        "    def __init__(self, state_size, action_size, args):\n",
        "        self.state_size = state_size  \n",
        "        self.action_size = action_size\n",
        "        self.batch_size = args['batch_size']\n",
        "        self.learning_rate = args['learning_rate']\n",
        "        self.num_nodes = args['number_nodes']    # number of nodes in each layer of NN\n",
        "        self.model = self.build_model()\n",
        "        self.model_ = self.build_model()\n",
        "        self.input_shape = 3*args['uav_number'] + 6\n",
        "        self.output_shape = args['uav_number'] + 2\n",
        "    \n",
        "    def build_model(self):\n",
        "        \n",
        "        # x is the input to the network \n",
        "        x = Input(shape=(3*args['uav_number'] + 6,))\n",
        "\n",
        "        # a series of fully connected layer for estimating Q(s,a) (value of actions from that state)\n",
        "\n",
        "        y1 = Dense(self.num_nodes, activation='relu')(x)\n",
        "        y2 = Dense(self.num_nodes, activation='relu')(y1)\n",
        "        z = Dense(args['uav_number'] + 2, activation=\"softmax\")(y2)\n",
        "\n",
        "        model = Model(inputs=x, outputs=z)\n",
        "        optimizer = adam_v2.Adam(learning_rate=self.learning_rate)\n",
        "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "        return model\n",
        "        \n",
        "    \n",
        "    def train(self, x, y, sample_weight=None, epochs=1, verbose=0):  #x is the input to the network and y is the output\n",
        "        self.model.fit(x, y, batch_size=len(x), sample_weight=sample_weight, epochs=epochs, verbose=verbose)\n",
        "        \n",
        "        \n",
        "     \n",
        "    def predict(self, state, target=False):\n",
        "        \n",
        "        if target:  # get prediction from target network\n",
        "            return self.model_.predict(state)\n",
        "        else: \n",
        "            return self.model.predict(state)\n",
        "        \n",
        "    \n",
        "    def predict_one_sample(self, state, target=False):   # used for exploitation\n",
        "       \n",
        "        self.predict(state, target=target)\n",
        "        \n",
        "        \n",
        "    def update_target_model(self):\n",
        "        self.model_.set_weights(self.model.get_weights())\n",
        "           \n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c96dbb8",
      "metadata": {
        "id": "7c96dbb8"
      },
      "outputs": [],
      "source": [
        "# state_size = 3*(5)+2\n",
        "# action_size = 7\n",
        "\n",
        "# obj = NeuralNetwork(state_size, action_size, args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eea1da75",
      "metadata": {
        "id": "eea1da75"
      },
      "source": [
        "### SumTree\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c3c9935",
      "metadata": {
        "id": "7c3c9935"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "\n",
        "class SumTree(object):\n",
        "    \n",
        "    def __init__(self, capacity):\n",
        "        self.write = 0 \n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2*capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "        \n",
        "    def add(self, priority, data):\n",
        "        idx = self.write + self.capacity - 1\n",
        "        \n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, priority)\n",
        "        \n",
        "        self.write += 1\n",
        "        \n",
        "        # overwrite to the first index if the memory capacity is completed\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "            \n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "            \n",
        "    def update(self, idx, priority):\n",
        "        change = priority - self.tree[idx]\n",
        "        \n",
        "        self.tree[idx] = priority\n",
        "        \n",
        "        # propagate the change through tree\n",
        "        while idx !=0:\n",
        "            idx = (idx -1) // 2\n",
        "            self.tree[idx] +=change\n",
        "     \n",
        "    def retrieve(self, idx, s):\n",
        "        left = 2*idx + 1\n",
        "        right = left + 1\n",
        "        \n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "        \n",
        "        if s<= self.tree[left]:\n",
        "            return self.retrieve(left, s)\n",
        "        else : \n",
        "            return self.retrieve(right, s - self.tree[left])\n",
        "        \n",
        "            \n",
        "            \n",
        "    def get(self, s):\n",
        "        idx = self.retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "        \n",
        "        return idx, self.tree[idx], self.data[dataIdx]   \n",
        "        # here returning leaf index, priority value and experience of that leaf index\n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "584ecf2a",
      "metadata": {
        "id": "584ecf2a"
      },
      "outputs": [],
      "source": [
        "#obj = SumTree(args['memory_capacity'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012c38b1",
      "metadata": {
        "id": "012c38b1"
      },
      "source": [
        "### PriortizedExperienceReplay\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e2e72d",
      "metadata": {
        "id": "99e2e72d"
      },
      "outputs": [],
      "source": [
        "import random  \n",
        "\n",
        "class ReplayMemory(object):\n",
        "    e = 0.05    # to avoid 0 probability of experiences \n",
        "    \n",
        "    def __init__(self, capacity, priority_scale):\n",
        "        self.capacity = capacity \n",
        "        self.priority_scale = priority_scale      # a in formula, used for balance b/w high priority and random sampling\n",
        "        self.max_priority = 0 \n",
        "        \n",
        "        self.memory = SumTree(self.capacity)    \n",
        "       \n",
        "    def get_priority(self, TDerror):\n",
        "        return (TDerror + self.e) ** self.priority_scale\n",
        "    \n",
        "    def remember(self, sample, TDerror):\n",
        "        priority = self.get_priority(TDerror)\n",
        "        self_max = max(self.max_priority, priority)\n",
        "        self.memory.add(self_max, sample)\n",
        "        \n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        sample_batch = []\n",
        "        sample_batch_indices = []\n",
        "        sample_batch_priorities = []\n",
        "        \n",
        "        num_segments = self.memory.total() / batch_size\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            left = num_segments * i \n",
        "            right = num_segments * (i + 1)\n",
        "            \n",
        "            s = random.uniform(left, right)\n",
        "            idx, priority, data = self.memory.get(s)\n",
        "            \n",
        "            sample_batch.append((idx,data))\n",
        "            sample_batch_indices.append(idx)\n",
        "            sample_batch_priorities.append(priority)\n",
        "            \n",
        "        return [sample_batch, sample_batch_indices, sample_batch_priorities]\n",
        "    \n",
        "    \n",
        "    def update(self, batch_indices, errors):\n",
        "        \n",
        "        for i in range(len(batch_indices)):\n",
        "            priority = self.get_priority(errors[i])\n",
        "            self.memory.update(batch_indices[i], priority)\n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2976c8a6",
      "metadata": {
        "id": "2976c8a6"
      },
      "outputs": [],
      "source": [
        "#obj = ReplayMemory(args['memory_capacity'], 0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f766834",
      "metadata": {
        "id": "2f766834"
      },
      "source": [
        "### Agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c40fac",
      "metadata": {
        "id": "42c40fac"
      },
      "outputs": [],
      "source": [
        "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "    \n",
        "    def __init__(self, state_size, action_size, agent_idx, arguments):\n",
        "        \n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.agent_idx = agent_idx\n",
        "        \n",
        "        self.learning_rate = arguments['learning_rate']\n",
        "        self.update_target_frequency = arguments['target_frequency']\n",
        "        self.batch_size = arguments[\"batch_size\"]\n",
        "        \n",
        "    \n",
        "        self.gamma = arguments[\"gamma\"]\n",
        "        self.epsilon = arguments['epsilon']\n",
        "        self.min_epsilon = arguments['min_epsilon']\n",
        "        self.epsilon_decay = arguments['epsilon_decay']\n",
        "        self.beta = arguments['min_beta']\n",
        "        self.beta_max = arguments['beta_max']\n",
        "        \n",
        "        self.step = 0\n",
        "        \n",
        "        self.dqn_model = NeuralNetwork(self.state_size, self.action_size, arguments)\n",
        "        self.memory = ReplayMemory(arguments['memory_capacity'], arguments['priority_scale'])\n",
        "     \n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        self.step +=1\n",
        "        \n",
        "        if (self.epsilon > self.min_epsilon):\n",
        "            self.epsilon = self.min_epsilon + (self.epsilon - self.min_epsilon) * math.exp(-1. * self.step * self.epsilon_decay)\n",
        "            return self.epsilon\n",
        "        else : \n",
        "            return self.min_epsilon\n",
        "        \n",
        "        if self.beta < self.beta_max:\n",
        "            self.beta+=0.01\n",
        "            #self.beta = self.beta + (self.beta - self.beta_max) * math.exp(-1. * self.step * 0.01)  #@@@@@@@@@@@ incrementing beta by 0.01\n",
        "    \n",
        "    def choose_action(self, state):\n",
        "        exploration_rate = self.decay_epsilon()\n",
        "        \n",
        "        if exploration_rate > random.random():\n",
        "            return random.randrange(-1,args['uav_number']+1)  #explore\n",
        "        else : \n",
        "            return np.argmax(self.dqn_model.predict_one_sample(state))\n",
        "        \n",
        "                         \n",
        "    def batch_error(self, batch):   # batch = [[priority, sample]] = [[0, (states, actions, rewards, next_states, done)]]\n",
        "        \n",
        "        \n",
        "        batch_len = len(batch)\n",
        "       \n",
        "        states = np.array([batch[i][1][0] for i in range(batch_len)])\n",
        "        states_ = np.array([batch[i][1][3] for i in range(batch_len)])\n",
        "        \n",
        "        predict = []\n",
        "        predict_ =[]\n",
        "        predict_target = []\n",
        "        for i in range(batch_len):\n",
        "           \n",
        "            predict.append(self.dqn_model.predict(states[i]))\n",
        "            predict_.append(self.dqn_model.predict(states_[i]))\n",
        "            predict_target.append(self.dqn_model.predict(states_[i], target=True))\n",
        "            \n",
        "        x = np.zeros((batch_len, self.state_size))\n",
        "        \n",
        "        y = np.zeros ((batch_len, args['uav_number']+2))\n",
        "       \n",
        "        \n",
        "        errors = np.zeros(batch_len)\n",
        "        \n",
        "        for i in range(batch_len):\n",
        "            \n",
        "            o = batch[i][1]                   # tuple = (state, action, reward, next_state, done)\n",
        "           \n",
        "            state = o[0]                     #state array \n",
        "            \n",
        "            action = o[1][self.agent_idx] \n",
        "            \n",
        "            reward = o[2]      # reward value \n",
        "            \n",
        "            \n",
        "            next_state = o[3]    # next_state\n",
        "           \n",
        "            done = o[4]\n",
        "             \n",
        "            q_value = predict[i]\n",
        "            next_q_value = predict_target[i]\n",
        "            \n",
        "            if done: \n",
        "                target_q_value = reward\n",
        "            else: \n",
        "                target_q_value  = reward + self.gamma * np.amax(next_q_value)\n",
        "            \n",
        "            \n",
        "            x[i] = state\n",
        "            y[i] = action \n",
        "            \n",
        "            errors[i] = np.abs(target_q_value- np.amax(q_value))\n",
        "        \n",
        "     \n",
        "        return [x, y, errors]\n",
        "    \n",
        "    \n",
        "    def observe(self, sample):\n",
        "        \n",
        "        _, _, errors = self.batch_error([(0, sample)])\n",
        "        \n",
        "        self.memory.remember(sample, errors)\n",
        "        \n",
        "        \n",
        "  \n",
        "    \n",
        "    def replay(self):\n",
        "        \n",
        "        [batch, batch_idx, batch_priorities] = self.memory.sample(self.batch_size)\n",
        "        \n",
        "        x, y, errors = self.batch_error(batch)\n",
        "        \n",
        "        \n",
        "        normalized_batch_priorities = [float(i) / sum(batch_priorities) for i in batch_priorities]\n",
        "        \n",
        "        # b_values = importance sampling weights \n",
        "        b_values = [(self.batch_size * i) ** (-1 * self.beta) for i in normalized_batch_priorities]\n",
        "        \n",
        "        normalized_b_values = [float(i) / max(b_values) for i in range(len(b_values))]\n",
        "        \n",
        "        sample_weights = [errors[i] * normalized_b_values[i] for i in range(len(errors))]\n",
        "        \n",
        "        self.dqn_model.train(x, y, np.array(sample_weights))\n",
        "        \n",
        "        self.memory.update(batch_idx, errors)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def update_target_model(self):\n",
        "        if self.step % self.update_target_frequency == 0 : \n",
        "            self.dqn_model.update_target_model()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280b42ee",
      "metadata": {
        "scrolled": true,
        "id": "280b42ee"
      },
      "outputs": [],
      "source": [
        "# state_size = 3*(5)+6\n",
        "# action_size = 1000\n",
        "# device_i =10\n",
        "\n",
        "# obj = Agent(state_size, action_size, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce25293",
      "metadata": {
        "id": "7ce25293"
      },
      "source": [
        "### Maths()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "692d9b64",
      "metadata": {
        "id": "692d9b64"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class Maths(object):\n",
        "    \n",
        "    def __init__(self,args):\n",
        "        \n",
        "        self.num_agents = args['agent_number']\n",
        "        self.num_uav = args['uav_number']\n",
        "        self.grid_width = args['grid_width']\n",
        "        self.uav_height = args['uav_height']\n",
        "        self.uav_range = args['uav_range']\n",
        "        self.local_compute = args['local_compute']\n",
        "        self.uav_compute = args['uav_compute']\n",
        "        self.cloud_compute = args['cloud_compute']\n",
        "        self.reference_distance = args['reference_distance']\n",
        "        self.los_channel_power = args['los_channel_power']\n",
        "        self.uav_bandwidth = args['uav_bandwidth']\n",
        "        self.cloud_bandwidth = args['cloud_bandwidth']\n",
        "        self.uav_power = args['uav_power']\n",
        "        self.cloud_power = args['cloud_power']\n",
        "        self.noise_power = args['noise_power']\n",
        "        self.propagation_time_factor = args['propagation_time_factor']\n",
        "        self.local_energy_consumption_factor = args['local_energy_consumption_factor']\n",
        "        self.task_size = args['task_size']\n",
        "        self.cpu_cycle = args['cpu_cycle']\n",
        "        self.tolerant_delay = args['tolerant_delay']\n",
        "        self.punishment_factor = args['punishment_factor']\n",
        "        \n",
        "        \n",
        "\n",
        "        # value not given in paper\n",
        "        # -----------------------------------------------------------------\n",
        "        self.a = 0.2   \n",
        "        self.b = 0.3   \n",
        "        self.attenuation_coefficient = 0.2\n",
        "        self.nue_los = 0.3\n",
        "        self.nue_nlos = 0.3\n",
        "        self.cloud_channel_gain = 0.3   # H(t) used in equation 6     \n",
        "        #-------------------------------------------------------------------\n",
        "        \n",
        "        \n",
        "        \n",
        "            \n",
        "    def uav_channel_gain(self, uav_pos, iiot_pos):  # takes position of a uav and iiot_device.    \n",
        "        \n",
        "        #P_los = 1/(1+(self.a * math.exp(-self.b * (math.atan(self.uav_height/uav_pos[0]))) - self.a))      # [equation -1]\n",
        "        P_los = 0.63\n",
        "        P_nlos = 1 - P_los            \n",
        "        \n",
        "        a, b = uav_pos, iiot_pos\n",
        "        distance = ((a[0]-b[0])**2 + (a[1]-b[1])**2)**(1/2)\n",
        "        PL_los = self.los_channel_power * ((math.sqrt(self.uav_height**2 + (distance)**2)) **(-self.attenuation_coefficient)) * self.nue_los      # [equation-2]                                      \n",
        "        \n",
        "        PL_nlos = self.los_channel_power * ((math.sqrt(self.uav_height**2 + (distance)**2)) **(-self.attenuation_coefficient)) * self.nue_nlos      # [equation-3]\n",
        "    \n",
        "        h_channel_condition = (P_los * PL_los) + (P_nlos * PL_nlos)        # [equation - 4]\n",
        "            \n",
        "        return h_channel_condition\n",
        "                   \n",
        "    \n",
        "    def uav_energy_consumption(self, uav_pos, iiot_pos):\n",
        "        self.h_channel_condition = self.uav_channel_gain(uav_pos, iiot_pos)\n",
        "        \n",
        "    \n",
        "        v1 = 1 + ((self.uav_power * self.h_channel_condition) / (self.noise_power) )    # value for log \n",
        "        uplink_transmission_rate = self.uav_bandwidth * (math.log(v1, 2))               # [equation - 5]  \n",
        "        \n",
        "        transmission_time = self.task_size / uplink_transmission_rate     # [equation -7]\n",
        "          \n",
        "        computation_time = self.cpu_cycle / self.uav_compute     #[equation -11]\n",
        "                   \n",
        "                   \n",
        "        uav_energy = self.uav_power * (transmission_time + computation_time)   #[equation -12]\n",
        "        \n",
        "        return uav_energy\n",
        "                   \n",
        "                   \n",
        "                   \n",
        "    def cloud_energy_consumption(self):\n",
        "         \n",
        "        v1 = 1 + ((self.cloud_power * self.cloud_channel_gain) / (self.noise_power) ) \n",
        "        uplink_transmission_rate = self.cloud_bandwidth * (math.log(v1, 2))   #[equation -6]\n",
        "        \n",
        "        transmission_time = (self.task_size / uplink_transmission_rate) + self.propagation_time_factor     # [equation -8]\n",
        "                   \n",
        "        cloud_energy = self.cloud_power * (transmission_time + self.propagation_time_factor)   #[equation - 13]        \n",
        "                   \n",
        "        return cloud_energy\n",
        "                   \n",
        "                   \n",
        "    \n",
        "    def local_energy_consumption(self):\n",
        "        return self.local_energy_consumption_factor * (self.cpu_cycle ** 2)      #[equation - 10]\n",
        "    \n",
        "                   \n",
        "                   \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdbb1b96",
      "metadata": {
        "id": "bdbb1b96"
      },
      "outputs": [],
      "source": [
        "#obj = Maths(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5444a4ff",
      "metadata": {
        "id": "5444a4ff"
      },
      "source": [
        "### Environment() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b11c2ef",
      "metadata": {
        "id": "2b11c2ef"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Environment(object):\n",
        "    \n",
        "    def __init__(self,agrs ):\n",
        "        self.num_agents = args['agent_number']\n",
        "        self.num_uav = args['uav_number']\n",
        "        self.grid_width = args['grid_width']\n",
        "        self.uav_height = args['uav_height']\n",
        "        self.uav_range = args['uav_range']\n",
        "        self.local_compute = args['local_compute']\n",
        "        self.cloud_compute = args['cloud_compute']\n",
        "        self.reference_distance = args['reference_distance']\n",
        "        self.los_channel_power = args['los_channel_power']\n",
        "        self.uav_bandwidth = args['uav_bandwidth']\n",
        "        self.cloud_bandwidth = args['cloud_bandwidth']\n",
        "        self.uav_power = args['uav_power']\n",
        "        self.cloud_power = args['cloud_power']\n",
        "        self.noise_power = args['noise_power']\n",
        "        self.propagation_time_factor = args['propagation_time_factor']\n",
        "        self.local_energy_consumption_factor = args['local_energy_consumption_factor']\n",
        "        self.task_size = args['task_size']\n",
        "        self.cpu_cycle = args['cpu_cycle']\n",
        "        self.tolerant_delay = args['tolerant_delay']\n",
        "        self.punishment_factor = args['punishment_factor']\n",
        "        self.uav_compute = args[\"uav_compute\"]\n",
        "        \n",
        "        # -----------------------\n",
        "        # value not given in paper\n",
        "        self.a = 0.2   \n",
        "        self.b = 0.3   \n",
        "        self.attenuation_coefficient = 0.2\n",
        "        self.nue_los = 0.3\n",
        "        self.nue_nlos = 0.3\n",
        "        self.cloud_channel_gain = 0.3   # H(t) used in equation 6     \n",
        "        # ------------------------\n",
        "        \n",
        "        \n",
        "        self.action_space = np.arange(-1, self.num_uav+1)   # action_space = {-1, 0, 1, ...., N}\n",
        "        self.users_space = np.zeros([self.num_agents], np.int32)\n",
        "        self.users_observation = np.zeros([self.num_agents], np.int32)\n",
        "        self.state_size = 3*(self.num_uav) + 6\n",
        "        self.action_size = args['uav_number']+2    \n",
        "        self.cloud_channel_gain = 0.3   # H(t) used in equation 6 \n",
        "       \n",
        "        self.UAVs_pos = self.UAVs_Position()\n",
        "        self.iiot_pos=self.IIots_Position() \n",
        "        self.terminal = False\n",
        "        \n",
        "        self.Maths = Maths(args)\n",
        "        \n",
        "    \n",
        "    def UAVs_Position(self): # choose random (x,y) position of UAV on grid with constant ht=uav_height \n",
        "        UAVs_pos = {}\n",
        "        x = random.sample(range(self.grid_width),self.num_uav)\n",
        "        y = random.sample(range(self.grid_width),self.num_uav)\n",
        "        \n",
        "        for i in range(1,self.num_uav+1):\n",
        "            point = (x[i-1],y[i-1],self.uav_height)\n",
        "            UAVs_pos[i] = point\n",
        "        return  UAVs_pos    # list of uav_positions\n",
        "    \n",
        "     \n",
        "        \n",
        "    def IIots_Position(self): # choose random (x,y) position of iiot on grid\n",
        "        iiot_pos = {}\n",
        "        for i in range(self.num_agents):\n",
        "            x = random.randint(1, self.grid_width)\n",
        "            y = random.randint(1, self.grid_width)\n",
        "            point = (x,y)\n",
        "            iiot_pos[i] = point   # list of iiot_positions \n",
        "        return iiot_pos \n",
        "    \n",
        "    \n",
        "    def state(self):     # for each iiot_device state vector is different because it takes the current position of iiot_device\n",
        "        uav_pos = [self.UAVs_pos[i] for i in range(1, self.num_uav+1)]\n",
        "        l = []\n",
        "        for i in uav_pos:\n",
        "            l.append(i[0])\n",
        "            l.append(i[1])\n",
        "            \n",
        "        all_states = []\n",
        "\n",
        "        \n",
        "        for i in range(self.num_agents):\n",
        "\n",
        "            state = [self.task_size, self.cpu_cycle, self.tolerant_delay, self.iiot_pos[i][0], self.iiot_pos[i][1]]\n",
        "            state.extend(l) # l =  uav_pos\n",
        "            state.append(self.cloud_channel_gain) \n",
        "            h_channel_condition = [self.Maths.uav_channel_gain(j, self.iiot_pos[i]) for j in uav_pos] \n",
        "            state.extend(h_channel_condition)\n",
        "            \n",
        "            state = np.array(state)\n",
        "            state = state.reshape(1,(args['uav_number']*3)+6)\n",
        "            #state = tf.convert_to_tensor(state)\n",
        "            all_states.append(state)\n",
        "        # print(all_states)    \n",
        "        return  all_states    #@@@@@@@@@@@@@@@@@@@@@@ is one_hot encoding required? \n",
        "         \n",
        "            \n",
        "            \n",
        "            \n",
        "    def reset(self):\n",
        "        self.UAV_Pos()\n",
        "        self.IIOT_Pos()\n",
        "        \n",
        "    \n",
        "    def step(self, actions):  # it takes a list of actions \n",
        "        rewards = []\n",
        "        for i,j in enumerate(actions): \n",
        "                if j > 0:               # uav task\n",
        "                    r = self.reward(j, self.UAVs_pos[j], self.iiot_pos[i])\n",
        "                \n",
        "                elif j == 0:\n",
        "                    r = self.reward(j)\n",
        "                \n",
        "                else : \n",
        "                    r = self.reward(j)\n",
        "                \n",
        "                rewards.append(r)\n",
        "        \n",
        "                \n",
        "        if sum(rewards):      #@@@@@@@@@@@@@@@@@@@@@@  any specific condition? \n",
        "            self.terminal = [False for i in range(len(actions))]\n",
        "        else: \n",
        "            self.terminal = True\n",
        "            \n",
        "        next_state = self.state()     #@@@@@@@@@@@@@@@@@@ how to get next_state?\n",
        "        \n",
        "        return next_state, rewards, self.terminal\n",
        "                    \n",
        "                    \n",
        "     \n",
        "    \n",
        "    \n",
        "    def reward_calculate(self, energy_consumption):\n",
        "        \n",
        "        if energy_consumption <= self.tolerant_delay: \n",
        "            return 1/energy_consumption\n",
        "        else:\n",
        "            return (1/energy_consumption)- self.punishment_factor \n",
        "        \n",
        "        \n",
        "    def reward(self, agent_action, uav_pos = None, iiot_pos = None):  ## Maths.uav_energy_consumption takes 2 parameters\n",
        "        \n",
        "        if agent_action == -1:   # offload to cloud\n",
        "            energy_consumption = Maths.cloud_energy_consumption(self)   \n",
        "            return self.reward_calculate(energy_consumption)\n",
        "        \n",
        "        elif agent_action == 0:  # compute locally\n",
        "            energy_consumption = Maths.local_energy_consumption(self)\n",
        "            return self.reward_calculate(energy_consumption)\n",
        "        \n",
        "        else: # offload to UAV\n",
        "            obj = Maths(args)\n",
        "            energy_consumption = obj.uav_energy_consumption(uav_pos, iiot_pos)\n",
        "            return self.reward_calculate(energy_consumption)\n",
        "        \n",
        "    def sample(self):\n",
        "        x =  np.random.choice(self.action_space,size=self.num_agents)\n",
        "        return x   \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ENV class"
      ],
      "metadata": {
        "id": "6SyYdRVf10Zv"
      },
      "id": "6SyYdRVf10Zv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a267687c",
      "metadata": {
        "id": "a267687c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ENV(object):\n",
        "    def __init__(self, args):\n",
        "        self.action_space = np.arange(-1, args['uav_number']+1)   # action_space = {-1, 0, 1, ...., N}\n",
        "        self.users_space = np.zeros([args['agent_number']], np.int32)\n",
        "        self.users_observation = np.zeros([args['agent_number']], np.int32)\n",
        "        self.state_size = 3*(args['uav_number']) + 6\n",
        "        self.action_size = args['uav_number']+2\n",
        "        self.env = Environment(args)\n",
        "        self.dqn_model = NeuralNetwork(self.state_size, self.action_size, args)\n",
        "        self.step_b_updates = 1\n",
        "        \n",
        "    def main(self, agents): # MDSPR Algorithm\n",
        "        total_step = 0\n",
        "        rewards_list = []\n",
        "\n",
        "        for episode in range(args['episodes']): \n",
        "\n",
        "            state = self.env.state()  # list of initial state vector for all iiot_devices.\n",
        "            done = False\n",
        "            reward_all = []\n",
        "            time_step = 0\n",
        "            \n",
        "           \n",
        "            while not done and time_step < args['max_timesteps']:\n",
        "                \n",
        "                actions = []   \n",
        "                    \n",
        "                for i in range(args['agent_number']): # using epsilon greedy to choose the action\n",
        "                    actions.append(agents[i].choose_action(state[i])) # appending action of each agent into actions list. \n",
        "                \n",
        "                \n",
        "                next_state, reward, done = self.env.step(actions)\n",
        "        \n",
        "                for i in range(len(agents)): # experience replay\n",
        "            \n",
        "                    agents[i].observe((state[i], actions, reward[i], next_state[i], done[i]))\n",
        "\n",
        "                    agents[i].decay_epsilon()\n",
        "\n",
        "                   \n",
        "                    \n",
        "                    \n",
        "                    if time_step % self.step_b_updates  and (time_step!=0)== 0 : \n",
        "                        \n",
        "                        agents[i].replay()\n",
        "\n",
        "                    if time_step % args['target_frequency'] and (time_step!=0) == 0:\n",
        "                       \n",
        "                        agents[i].update_target_model()\n",
        "                        \n",
        "                #print(\"total_step : \", total_step,\"\\n\\n\")\n",
        "                time_step +=1\n",
        "                total_step +=1\n",
        "                state = next_state\n",
        "                reward_all.append(reward)\n",
        "\n",
        "            rewards_list.append(reward_all)\n",
        "            print(f\"episode {episode} -----reward : {reward_all} --------Final step : {time_step}, --------done : {done}\\n\\n\\n\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Function"
      ],
      "metadata": {
        "id": "S8v3ThD214rx"
      },
      "id": "S8v3ThD214rx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cafca931",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cafca931",
        "outputId": "b9380f21-2c09-482c-a4b3-c613c9831f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 0 -----reward : [[232685.67540325108, -1.7388674208078102e-07, -1.10621343691578e-11]] --------Final step : 1, --------done : [False, False, False]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode 1 -----reward : [[-1.3493542511731201e-11, -1.3000350719590717e-11, -1.0842828372759324e-11]] --------Final step : 1, --------done : [False, False, False]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode 2 -----reward : [[232685.67540325108, -1.0985940638149626e-11, -1.7388674208078102e-07]] --------Final step : 1, --------done : [False, False, False]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode 3 -----reward : [[-1.0930837430661354e-11, -1.3331017319407473e-11, 232685.67540325108]] --------Final step : 1, --------done : [False, False, False]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode 4 -----reward : [[-1.0930837430661354e-11, 232685.67540325108, -1.1893606080022056e-11]] --------Final step : 1, --------done : [False, False, False]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode 5 -----reward : [[-1.3493542511731201e-11, -1.088728552926226e-11, -1.10621343691578e-11]] --------Final step : 1, --------done : [False, False, False]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode 6 -----reward : [[-1.0930837430661354e-11, -1.3376913053906478e-11, -1.10621343691578e-11]] --------Final step : 1, --------done : [False, False, False]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode 7 -----reward : [[-1.0958807783628744e-11, -1.3376913053906478e-11, -1.0085205598525424e-11]] --------Final step : 1, --------done : [False, False, False]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode 8 -----reward : [[-1.3493542511731201e-11, -1.0985940638149626e-11, -1.10621343691578e-11]] --------Final step : 1, --------done : [False, False, False]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "episode 9 -----reward : [[-1.0930837430661354e-11, 232685.67540325108, -1.0842828372759324e-11]] --------Final step : 1, --------done : [False, False, False]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    \n",
        "    parser.add_argument(\"-f\")\n",
        "    parser.add_argument(\"-lr\", \"--learning_rate\", default=0.0001, type=float, help=\"learning rate\")\n",
        "    parser.add_argument(\"-tf\", \"--target_frequency\", default=2, type=int, help=\"target weights replace steps\")\n",
        "    parser.add_argument(\"-bs\", \"--batch_size\", default=50, type=int, help=\"batch size\")\n",
        "    parser.add_argument(\"-ga\", \"--gamma\", default=0.7, type=float, help=\"reward decay rate\")\n",
        "    parser.add_argument(\"-e\", \"--epsilon\", default=0.9, type=float, help=\"exploration rate\")\n",
        "    parser.add_argument(\"-c\", \"--memory_capacity\", default=10000, type=int, help=\"replay memory capacity\")\n",
        "    parser.add_argument(\"-nn\", \"--number_nodes\", default=100, type=int, help=\"number of nodes in each layer of neural network\")\n",
        "    parser.add_argument(\"-m\", \"--agent_number\", default=3, type=int, help=\"total number of iiot devices\")\n",
        "    parser.add_argument(\"-uav\", \"--uav_number\", default=5, type=int, help=\"total number of UAVs\")\n",
        "    parser.add_argument(\"-g\", \"--grid_width\", default=800, type=int, help=\"size of fixed area under consideration\")\n",
        "    parser.add_argument(\"-H\", \"--uav_height\" , default=100, type=int, help=\"flying height of UAV\")\n",
        "    parser.add_argument(\"-r\", \"--uav_range\", default=300, type=int, help=\"communication range of UAV\")\n",
        "    parser.add_argument(\"-cl\", \"--local_compute\", default=500, type=int, help=\"local computation capacity 'MHz'\")\n",
        "    parser.add_argument(\"-cu\", \"--uav_compute\", default=2, type=int, help=\"UAV compution capacity 'GHz'\")\n",
        "    parser.add_argument(\"-cc\", \"--cloud_compute\", default=100, type=int, help=\"cloud computation capacity 'GHz'\")\n",
        "    parser.add_argument(\"-rd\", \"--reference_distance\", default=1, type=float, help=\"channel gain reference distance 'meters'\")\n",
        "    parser.add_argument(\"-lcp\", \"--los_channel_power\", default=1.42e-4, type=float, help=\"channel gain at the reference\")\n",
        "    parser.add_argument(\"-ub\", \"--uav_bandwidth\", default=15, type=int, help=\"bandwidth allocated for UAV uplin transmission rate 'MHz'\")\n",
        "    parser.add_argument(\"-cb\", \"--cloud_bandwidth\", default=10, type=int, help=\"bandwidth allocated for cloud uplink transmission 'MHz'\")\n",
        "    parser.add_argument(\"-up\",\"--uav_power\", default=0.01, type=float, help=\"uplink transmission power for UAV offloading  'W'\")\n",
        "    parser.add_argument(\"-cp\", \"--cloud_power\", default=0.015, type=float, help=\"uplink transmission power for cloud offloading  'W'\")\n",
        "    parser.add_argument(\"-n\", \"--noise_power\", default=-90, type=float, help=\"background noise power  'dBm/Hz'\")\n",
        "    parser.add_argument(\"-ptf\", \"--propagation_time_factor\", default=4e-9, type=float, help=\"uplink propogation delay factor  's/bit'\")\n",
        "    parser.add_argument(\"-lec\", \"--local_energy_consumption_factor\", default=1e-23, type=float, help=\"local energy consumption factor 'theta' J/cycle\")\n",
        "    \n",
        "    \n",
        "    # values not given in paper\n",
        "    #------------------------------------------------------------------------------------------------------------------------------------\n",
        "    parser.add_argument(\"-pf\", \"--punishment_factor\", default=-2, type = float, help=\"if tolerant delay < energy consumption\")  \n",
        "    parser.add_argument(\"-p\", \"--priority_scale\", default=0.4, type=float, help=\"scale for prioritization\")  \n",
        "    parser.add_argument(\"-m_e\", \"--min_epsilon\", default=0.02, type=float, help=\"minimum value of exploration rate\")\n",
        "    parser.add_argument(\"-e_d\", \"--epsilon_decay\", default=0.0001, type=float, help=\"exploration decay rate\")\n",
        "    parser.add_argument('-m_b', \"--min_beta\", default=0.4, type=float, help=\"minimum value of importance sampling\")\n",
        "    parser.add_argument(\"-b_max\", \"--beta_max\", default=1.0, type=float, help=\"incrementing value of importance sampling beta\")\n",
        "    parser.add_argument(\"-ts\", \"--max_timesteps\", default=20, type=int, help=\"maximum timesteps in each epsisode\")  ## value not given in paper\n",
        "    parser.add_argument(\"-ed\", \"--episodes\", default=10 , type=int, help=\"total number of episodes\")    ## value not given in paper \n",
        "    #-------------------------------------------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    \n",
        "    \n",
        "    # values range is given in paper\n",
        "    # ----------------------------------------------------------------------------------------------------------------------------------------\n",
        "    parser.add_argument(\"-t\", \"--task_size\", default=random.uniform(100, 800000), type=float, help=\"offloading task size  'Kb'\")\n",
        "    parser.add_argument(\"-cpu\", \"--cpu_cycle\", default=random.uniform(5e+5, 5e+9), type=float, help=\"cpu_cycle required by task \")\n",
        "    parser.add_argument(\"-d\", \"--tolerant_delay\", default=random.uniform(0.01, 1), type=float, help=\"task tolerant value 'seconds'\")\n",
        "    #------------------------------------------------------------------------------------------------------------------------------------------\n",
        "    np.set_printoptions(precision=2)\n",
        "\n",
        "    args = vars(parser.parse_args())\n",
        "    \n",
        "    \n",
        "    env = ENV(args)\n",
        "   \n",
        "    \n",
        "    all_agents = []\n",
        "    for agent_idx in range(args['agent_number']):\n",
        "        all_agents.append(Agent(env.state_size, env.action_size, agent_idx, args))\n",
        "    \n",
        "    env.main(all_agents)\n",
        "    \n",
        "   "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Multi-agent deep Q-network with stochastic prioritized replay .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NfY4FtkL1j9x",
        "ddf465e9",
        "eea1da75",
        "012c38b1",
        "2f766834",
        "7ce25293",
        "5444a4ff",
        "6SyYdRVf10Zv",
        "S8v3ThD214rx"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

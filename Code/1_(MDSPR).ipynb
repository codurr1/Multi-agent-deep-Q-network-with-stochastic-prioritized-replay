{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62239b8",
   "metadata": {
    "id": "f62239b8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import argparse  \n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from collections import deque\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "#https://stackoverflow.com/questions/69136518/tensorflow-2-getting-warningtensorflowx-out-of-the-last-x-calls-to-function\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13603fb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13603fb8",
    "outputId": "ff0b4554-113e-4061-c982-975aefcbee79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version :  2.8.0\n",
      "Python 3.9.7\n"
     ]
    }
   ],
   "source": [
    "print(\"tf version : \",tf.__version__)\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe027a50",
   "metadata": {
    "id": "fe027a50"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddf465e9",
   "metadata": {
    "id": "ddf465e9"
   },
   "source": [
    "### NeuralNetwork\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4178e040",
   "metadata": {
    "id": "4178e040",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, input_shape, action_size,args):\n",
    "        \n",
    "        \n",
    "        self.state_size = input_shape\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = args['learning_rate']\n",
    "        self.num_nodes = args['number_nodes']    \n",
    "        self.model = self.build_model()\n",
    "        self.model_ = self.build_model()\n",
    "         \n",
    "   \n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        # x is the input to the network \n",
    "        x = Input(self.state_size)\n",
    "        \n",
    "        if args['dueling']: # dueling-DQN \n",
    "            \n",
    "            # for V(s)\n",
    "            a1 = Dense(self.num_nodes, activation=\"relu\")(x)\n",
    "            a2 = Dense(self.num_nodes, activation=\"relu\")(a1)\n",
    "            a3 = Dense(1, activation=\"linear\")(a2)\n",
    "            \n",
    "            # for A(s,a)\n",
    "            b1 = Dense(self.num_nodes, activation=\"relu\")(x)\n",
    "            b2 = Dense(self.num_nodes, activation=\"relu\")(b1)\n",
    "            b3 = Dense(args['uav_number']+2, activation=\"linear\")(b2)\n",
    "            \n",
    "            c = Concatenate(axis=-1)([a3, b3])\n",
    "            \n",
    "            # for Q(s,a)\n",
    "            z = Lambda(lambda a: K.expand_dims(a[:, 0], axis=-1) + a[:, 1:] - K.mean(a[:, 1:], keepdims=True),\n",
    "                      output_shape=(self.action_size, ))(c)\n",
    "        \n",
    "        \n",
    "        else :  # standard DQN \n",
    "            \n",
    "            y1 = Dense(self.num_nodes, input_shape =self.state_size, activation='relu')(x)\n",
    "            y2 = Dense(self.num_nodes, activation='relu')(y1)\n",
    "            z = Dense(args['uav_number'] + 2, activation=\"linear\")(y2)\n",
    "\n",
    "        model = Model(inputs=x, outputs=z)\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "        #model.summary()\n",
    "        return model\n",
    "        \n",
    "   \n",
    "    def train(self, x, y, sample_weight=None, epochs=100, verbose=0):  #x is the input to the network and y is the output\n",
    "        self.model.fit(x, y, batch_size=len(x), sample_weight=sample_weight, epochs=epochs, verbose=verbose)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self, state, target=False):\n",
    "        \n",
    "        if target: \n",
    "            return self.model_.predict(state)\n",
    "        else: \n",
    "            return self.model.predict(state )\n",
    "        \n",
    "    \n",
    "    def predict_one_sample(self, state, target=False):   \n",
    "       \n",
    "        self.predict(state, target=target )\n",
    "        \n",
    "        \n",
    "    def update_target_model(self):\n",
    "        self.model_.set_weights(self.model.get_weights())\n",
    "           \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e2bbe",
   "metadata": {
    "id": "0b9e2bbe"
   },
   "source": [
    "### Uniform Experience Replay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d22c01a",
   "metadata": {
    "id": "0d22c01a"
   },
   "outputs": [],
   "source": [
    "class UniformReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen = self.capacity)\n",
    "        \n",
    "    def uer_remember(self, sample):\n",
    "        \n",
    "        self.memory.append(sample)\n",
    "        \n",
    "    def uer_sample(self, batch_size):\n",
    "        \n",
    "        batch_size = min(batch_size, len(self.memory))\n",
    "        sample_batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        return sample_batch\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1da75",
   "metadata": {
    "id": "eea1da75"
   },
   "source": [
    "### SumTree\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c3c9935",
   "metadata": {
    "id": "7c3c9935"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SumTree(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.write = 0 \n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2*capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        \n",
    "        \n",
    " \n",
    "    def add(self, priority, data):\n",
    "        \n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, priority)\n",
    "        self.write += 1\n",
    "\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "            \n",
    "            \n",
    "       \n",
    "    \n",
    "            \n",
    "    def total(self):\n",
    "        return self.tree[0]    \n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def update(self, idx, priority):\n",
    "        \n",
    "        change = priority - self.tree[idx]\n",
    "        self.tree[idx] = priority\n",
    "        \n",
    "        while idx !=0:\n",
    "            idx = (idx -1) // 2\n",
    "            self.tree[idx] +=change\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "     \n",
    "    def retrieve(self, idx, s):\n",
    "        \n",
    "        left = 2*idx + 1\n",
    "        right = left + 1\n",
    "        \n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        \n",
    "        if s<= self.tree[left]:\n",
    "            return self.retrieve(left, s)\n",
    "        \n",
    "        else : \n",
    "            return self.retrieve(right, s - self.tree[left])\n",
    "        \n",
    "            \n",
    "            \n",
    "    def get(self, s):\n",
    "        \n",
    "        idx = self.retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        \n",
    "        return idx, self.tree[idx], self.data[dataIdx]   \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c38b1",
   "metadata": {
    "id": "012c38b1"
   },
   "source": [
    "### PriortizedExperienceReplay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e2e72d",
   "metadata": {
    "id": "99e2e72d"
   },
   "outputs": [],
   "source": [
    "import random  \n",
    "\n",
    "class PrioritizedReplayMemory(object):\n",
    "    \n",
    "     \n",
    "    def __init__(self, capacity, priority_scale):\n",
    "        \n",
    "        self.capacity = capacity \n",
    "        self.priority_scale = priority_scale     \n",
    "        self.max_priority = 0 \n",
    "        self.e = 0.01    # to avoid 0 probability of experiences\n",
    "        self.memory = SumTree(self.capacity)    \n",
    "     \n",
    "    \n",
    "    \n",
    "    def get_priority(self, TDerror):\n",
    "        \n",
    "        return (TDerror + self.e) ** self.priority_scale\n",
    "    \n",
    "    \n",
    "    \n",
    "    def priority_remember(self, sample, TDerror):\n",
    "       \n",
    "        priority = self.get_priority(TDerror)\n",
    "        self_max = max(self.max_priority, priority)\n",
    "        self.memory.add(self_max, sample)\n",
    "\n",
    "     \n",
    "    \n",
    "    def priority_sample(self, batch_size):\n",
    "        \n",
    "        sample_batch = []\n",
    "        sample_batch_indices = []\n",
    "        sample_batch_priorities = []\n",
    "        \n",
    "        num_segments = self.memory.total() / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            left = num_segments * i \n",
    "            right = num_segments * (i + 1)\n",
    "            s = random.uniform(left, right)\n",
    "            idx, priority, data = self.memory.get(s)\n",
    "            \n",
    "            sample_batch.append((idx,data))\n",
    "            sample_batch_indices.append(idx)\n",
    "            sample_batch_priorities.append(priority)\n",
    "            \n",
    "        return [sample_batch, sample_batch_indices, sample_batch_priorities]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def priority_update(self, batch_indices, errors):\n",
    "\n",
    "        for i in range(len(batch_indices)):\n",
    "            \n",
    "            priority = self.get_priority(errors[i])\n",
    "            self.memory.update(batch_indices[i], priority)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1b840",
   "metadata": {
    "id": "e4b1b840"
   },
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8d2758",
   "metadata": {
    "id": "1f8d2758"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, args):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = args['learning_rate']\n",
    "        self.update_target_frequency = args['target_frequency']\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.gamma = args[\"gamma\"]\n",
    "        self.epsilon = args['epsilon']\n",
    "        self.min_epsilon = args['min_epsilon']\n",
    "        self.epsilon_decay = args['epsilon_decay']\n",
    "        self.beta = args['min_beta']\n",
    "        self.beta_max = args['beta_max']\n",
    "        self.agent_num = args['agent_number']\n",
    "        self.beta_increment = args['beta_increment']\n",
    "        self.step = 0\n",
    "        self.dqn_model = NeuralNetwork(action_size = self.action_size, args = args, input_shape = (self.state_size, ))\n",
    "        self.target_dqn_model = NeuralNetwork(action_size = self.action_size, args = args, input_shape = (self.state_size, ))\n",
    "        \n",
    "        if args['memory_type']==\"per\":\n",
    "            \n",
    "            self.memory = PrioritizedReplayMemory(args['memory_capacity'], args['priority_scale'])\n",
    "            \n",
    "        else : \n",
    "            \n",
    "             self.memory = UniformReplayMemory(args['memory_capacity'])\n",
    "    \n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "       \n",
    "        self.step +=1\n",
    "        \n",
    "        if self.beta < self.beta_max:\n",
    "            \n",
    "            self.beta = self.beta_max + (self.beta - self.beta_max) * np.exp(-1. * self.step * self.beta_increment) \n",
    "           \n",
    "            \n",
    "        if (self.epsilon > self.min_epsilon):\n",
    "            \n",
    "            self.epsilon = self.min_epsilon + (self.epsilon - self.min_epsilon) * np.exp(-1. * self.step * self.epsilon_decay)\n",
    "           \n",
    "            return self.epsilon\n",
    "        \n",
    "        else : \n",
    "            \n",
    "            return self.min_epsilon\n",
    "        \n",
    "       \n",
    "    \n",
    "   \n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        exploration_rate = self.decay_epsilon()\n",
    "        \n",
    "        if exploration_rate > random.random():\n",
    "            \n",
    "            return random.randrange(-1,args['uav_number']+1)  #explore\n",
    "        \n",
    "        else : \n",
    "            \n",
    "            state = np.reshape(state, (1,args['uav_number']*3 +6))\n",
    "            \n",
    "            return np.argmax(self.dqn_model.predict_one_sample(state))\n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    def per_batch_error(self, batch):   # batch = [(0, sample)] = [(0, (states[i], actions[i], rewards[i], next_states[i], done[i]))]\n",
    "        \n",
    "        batch_len = len(batch)\n",
    "        \n",
    "        states = np.array([batch[i][1][0] for i in range(batch_len)])\n",
    "        next_states = np.array([batch[i][1][3] for i in range(batch_len)])\n",
    "        \n",
    "        action = [batch[i][1][1] for i in range(batch_len)]\n",
    "        reward = [batch[i][1][2] for i in range(batch_len)]\n",
    "        done =   [batch[i][1][4] for i in range(batch_len)]\n",
    "        \n",
    "        \n",
    "        target = NeuralNetwork(action_size = self.action_size, args = args, input_shape = (self.state_size, )).predict(states)\n",
    "\n",
    "        target_old = np.array(target)\n",
    "        \n",
    "        target_ = NeuralNetwork(action_size = self.action_size, args = args, input_shape = (self.state_size, )).predict(next_states)\n",
    "        \n",
    "        target_next = NeuralNetwork(action_size = self.action_size, args = args, input_shape = (self.state_size, )).predict(next_states, target=True)\n",
    "        \n",
    "        x = np.zeros((batch_len, self.state_size))\n",
    "        y = np.zeros ((batch_len, args['uav_number']+2))\n",
    "        loss = np.zeros(args['agent_number'])\n",
    "        errors = np.zeros(batch_len)\n",
    "       \n",
    "        for i in range(batch_len):\n",
    "           \n",
    "            if done[i]:\n",
    "                \n",
    "                target[i][action[i]] = reward[i]\n",
    "            else: \n",
    "                \n",
    "                if args['double_dqn'] == \"double_dqn\":\n",
    "                    \n",
    "                    target[i][action[i]] = reward[i] + self.gamma * target_next[i][np.argmax(target_[i])]\n",
    "                \n",
    "                else : \n",
    "                   \n",
    "                    target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "            x[i] = states[i]\n",
    "            y[i] = action[i]\n",
    "          \n",
    "        indices = np.arange(batch_len)\n",
    "        errors = np.abs(target_old[indices, np.array(action)] - target[indices, np.array(action)] )\n",
    "       \n",
    "        if batch_len == self.agent_num:\n",
    "            \n",
    "            loss = np.sqrt(errors)\n",
    "\n",
    "\n",
    "        return x, y, errors, loss\n",
    "\n",
    "      \n",
    "        \n",
    "    \n",
    "    def per_observe(self, sample):\n",
    "        \n",
    "        _, _, errors, loss= self.per_batch_error(sample)\n",
    "        \n",
    "        \n",
    "        for i in range(len(sample)):\n",
    "        \n",
    "            self.memory.priority_remember(sample[i][1], errors[i])\n",
    "        \n",
    "        return loss \n",
    "        \n",
    "  \n",
    "    \n",
    "    def per_replay(self):\n",
    "     \n",
    "        [batch, batch_idx, batch_priorities] = self.memory.priority_sample(self.batch_size)\n",
    "        print(\"# replay-------- \")\n",
    "        x, y, errors,_= self.per_batch_error(batch)\n",
    "       \n",
    "        normalized_batch_priorities = [float(i) / sum(batch_priorities) for i in batch_priorities]      # P(i) = (pi)**a / sum(pk)**a where pi is priority value & \n",
    "                                                                                                        # sum(pk) normalization by all priority values in replay buffer\n",
    "        \n",
    "        # b_values = importance sampling weights \n",
    "        b_values = [(self.batch_size * i) ** (-1 * self.beta) for i in normalized_batch_priorities]     # (1/N * 1/P(i))**b\n",
    "        \n",
    "        normalized_b_values = [float(i) / max(b_values) for i in range(len(b_values))]\n",
    "        \n",
    "        sample_weights = [errors[i] * normalized_b_values[i] for i in range(len(errors))]\n",
    "        \n",
    "        self.dqn_model.train(x, y, np.array(sample_weights))\n",
    "        \n",
    "        self.memory.priority_update(batch_idx, errors)\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    def uer_batch_error(self, batch):   # batch = [(0, sample)] = [(0, (states[i], actions[i], rewards[i], next_states[i], done[i]))]\n",
    "        \n",
    "        batch_len = len(batch)\n",
    "        \n",
    "        states = np.array([batch[i][0] for i in range(batch_len)])\n",
    "        next_states = np.array([batch[i][3] for i in range(batch_len)])\n",
    "        \n",
    "        action = [batch[i][1] for i in range(batch_len)]\n",
    "        reward = [batch[i][2] for i in range(batch_len)]\n",
    "        done =   [batch[i][4] for i in range(batch_len)]\n",
    "        \n",
    "        \n",
    "        target = NeuralNetwork(action_size = self.action_size, args = args, input_shape = (self.state_size, )).predict(states)\n",
    "\n",
    "        target_old = np.array(target)\n",
    "        \n",
    "        target_ = NeuralNetwork(action_size = self.action_size, args = args, input_shape = (self.state_size, )).predict(next_states)\n",
    "\n",
    "        target_next = NeuralNetwork(action_size = self.action_size, args = args, input_shape = (self.state_size, )).predict(next_states, target=True)\n",
    "        \n",
    "        x = np.zeros((batch_len, self.state_size))\n",
    "        y = np.zeros ((batch_len, args['uav_number']+2))\n",
    "        loss = np.zeros(args['agent_number'])\n",
    "        errors = np.zeros(batch_len)\n",
    "        \n",
    "        for i in range(batch_len):\n",
    "           \n",
    "            if done[i]:\n",
    "                \n",
    "                target[i][action[i]] = reward[i]\n",
    "                \n",
    "            else: \n",
    "                \n",
    "                if args['double_dqn']==\"double_dqn\":\n",
    "                    \n",
    "                    target[i][action[i]] = reward[i] + self.gamma * target_next[i][np.argmax(target_[i])]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "            x[i] = states[i]\n",
    "            y[i] = action[i]\n",
    "          \n",
    "        indices = np.arange(batch_len)\n",
    "        errors = np.abs(target_old[indices, np.array(action)] - target[indices, np.array(action)] )\n",
    "       \n",
    "        if batch_len == self.agent_num:\n",
    "            \n",
    "            loss = np.sqrt(errors)\n",
    "\n",
    "\n",
    "        return x, y, loss\n",
    "\n",
    "      \n",
    "        \n",
    "    \n",
    "    def uer_observe(self, sample):\n",
    "        \n",
    "        _, _, loss= self.uer_batch_error(sample)\n",
    "        \n",
    "        for i in range(len(sample)):\n",
    "\n",
    "                self.memory.uer_remember(sample[i])\n",
    "        \n",
    "        return loss \n",
    "        \n",
    "  \n",
    "    \n",
    "    def uer_replay(self):\n",
    "     \n",
    "        batch = self.memory.uer_sample(self.batch_size)\n",
    "        x, y, _ = self.uer_batch_error(batch)\n",
    "        self.dqn_model.train(x,y)\n",
    "        \n",
    "        \n",
    "    def update_target_model(self):\n",
    "        \n",
    "        if self.step % self.update_target_frequency == 0 : \n",
    "            self.dqn_model.update_target_model()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce25293",
   "metadata": {
    "id": "7ce25293"
   },
   "source": [
    "### Maths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692d9b64",
   "metadata": {
    "id": "692d9b64"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class Maths(object):\n",
    "    \n",
    "    def __init__(self,args):\n",
    "        \n",
    "        self.num_agents = args['agent_number']\n",
    "        self.num_uav = args['uav_number']\n",
    "        self.grid_width = args['grid_width']\n",
    "        self.uav_height = args['uav_height']\n",
    "        self.uav_range = args['uav_range']\n",
    "        self.local_compute = args['local_compute']\n",
    "        self.uav_compute = args['uav_compute']\n",
    "        self.cloud_compute = args['cloud_compute']\n",
    "        self.reference_distance = args['reference_distance']\n",
    "        self.los_channel_power = args['los_channel_power']\n",
    "        self.uav_bandwidth = args['uav_bandwidth']\n",
    "        self.cloud_bandwidth = args['cloud_bandwidth']\n",
    "        self.uav_power = args['uav_power']\n",
    "        self.cloud_power = args['cloud_power']\n",
    "        self.noise_power = args['noise_power']\n",
    "        self.propagation_time_factor = args['propagation_time_factor']\n",
    "        self.local_energy_consumption_factor = args['local_energy_consumption_factor']\n",
    "        self.punishment_factor = args['punishment_factor']\n",
    "        self.cloud_channel_gain = args['cloud_channel_gain']   # H(t) used in equation 6 \n",
    "    \n",
    "    \n",
    "    \n",
    "    def uav_channel_gain(self, uav_pos, iiot_pos):\n",
    "        \n",
    "        a, b = uav_pos, iiot_pos\n",
    "    \n",
    "        distance = ((a[0]-b[0])**2 + (a[1]-b[1])**2)**(1/2)\n",
    "        \n",
    "        h_channel_condition = self.los_channel_power / ((self.uav_height**2) + (distance)**2)\n",
    "        \n",
    "        return h_channel_condition\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def uav_computation_time(self, uav_pos, iiot_pos, task_size, cpu_cycle):\n",
    "        \n",
    "        self.h_channel_condition = self.uav_channel_gain(uav_pos, iiot_pos)\n",
    "        \n",
    "        v1 = 1 + ((self.uav_power * self.h_channel_condition) / (self.noise_power) )    # value for log \n",
    "        uplink_transmission_rate = self.uav_bandwidth * (math.log(v1, 2))               # [equation - 5]  \n",
    "        \n",
    "        transmission_time = task_size / uplink_transmission_rate     # [equation -7]\n",
    "        \n",
    "        computation_time = cpu_cycle / self.uav_compute     #[equation -11] \n",
    "        \n",
    "        execution_time = transmission_time + computation_time   \n",
    "        \n",
    "        return  execution_time\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def cloud_transmission_time(self, task_size):\n",
    "        \n",
    "        v1 = 1 + ((self.cloud_power * self.cloud_channel_gain) / (self.noise_power) ) \n",
    "        \n",
    "        uplink_transmission_rate = self.cloud_bandwidth * (math.log(v1, 2))   #[equation -6]\n",
    "        \n",
    "        transmission_time = (task_size / uplink_transmission_rate) + self.propagation_time_factor     # [equation -8]\n",
    "\n",
    "        return transmission_time\n",
    "    \n",
    "    \n",
    "    def local_computation_time(self, cpu_cycle):\n",
    "        \n",
    "        local_computation_time = cpu_cycle / self.local_compute    # [equation-9]\n",
    "    \n",
    "        return local_computation_time\n",
    "        \n",
    "      \n",
    "\n",
    "    \n",
    "    def uav_energy_consumption(self, uav_pos, iiot_pos, task_size, cpu_cycle):\n",
    "        \n",
    "        execution_time= self.uav_computation_time(uav_pos, iiot_pos, task_size, cpu_cycle)           \n",
    "                   \n",
    "        uav_energy = self.uav_power * (execution_time)   #[equation -12]\n",
    "    \n",
    "        return uav_energy\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def cloud_energy_consumption(self, task_size):\n",
    "         \n",
    "        transmission_time = self.cloud_transmission_time( task_size)\n",
    "        cloud_energy = self.cloud_power * (transmission_time + self.propagation_time_factor)   #[equation - 13]\n",
    "        \n",
    "        return cloud_energy\n",
    "                   \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def local_energy_consumption(self, cpu_cycle):\n",
    "       \n",
    "        local_energy_consumption = self.local_energy_consumption_factor * (cpu_cycle ** 2)   #[equation - 10]\n",
    "        \n",
    "        return local_energy_consumption      \n",
    "    \n",
    "                   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5444a4ff",
   "metadata": {
    "id": "5444a4ff"
   },
   "source": [
    "### Environment() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b11c2ef",
   "metadata": {
    "id": "2b11c2ef"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Environment(object):\n",
    "    \n",
    "    def __init__(self,agrs ):\n",
    "        self.num_agents = args['agent_number']\n",
    "        self.num_uav = args['uav_number']\n",
    "        self.grid_width = args['grid_width']\n",
    "        self.uav_height = args['uav_height']\n",
    "        self.uav_range = args['uav_range']\n",
    "\n",
    "        self.task_size, self.cpu_cycle, self.tolerant_delay = self.task_model()\n",
    "        \n",
    "        self.action_space = np.arange(-1, self.num_uav+1)   # action_space = {-1, 0, 1, ...., N}\n",
    "        \n",
    "        self.users_observation = np.zeros([self.num_agents], np.int32)\n",
    "        self.state_size = 3*(self.num_uav) + 6\n",
    "        self.action_size = self.num_agents   \n",
    "       \n",
    "        self.UAVs_pos = self.UAVs_Position()\n",
    "        self.iiot_pos=self.IIots_Position() \n",
    "        \n",
    "        self.Maths = Maths(args)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def task_model(self):\n",
    "        \n",
    "        task_size = random.sample(range(100, 80000), self.num_agents)   # task_size is taken in \"Kb\".\n",
    "        cpu_cycle =random.sample(range(500000, 5000000000), self.num_agents)\n",
    "        tolerant_delay =[ round(random.uniform(0.1, 1),7) for i in range(self.num_agents)]\n",
    "        \n",
    "        return task_size, cpu_cycle, tolerant_delay\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def UAVs_Position(self):\n",
    "        \n",
    "        UAVs_pos = {}\n",
    "        x = random.sample(range(self.grid_width),self.num_uav)\n",
    "        y = random.sample(range(self.grid_width),self.num_uav)\n",
    "        \n",
    "        for i in range(1,self.num_uav+1):\n",
    "            \n",
    "            point = [x[i-1],y[i-1],self.uav_height]\n",
    "            UAVs_pos[i] = point\n",
    "            \n",
    "        return  UAVs_pos    # list of uav_positions\n",
    "    \n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "    def IIots_Position(self):\n",
    "            \n",
    "        iiot_pos = {}  \n",
    "        \n",
    "        for i in range(self.num_agents):\n",
    "            \n",
    "            x = random.randint(1, self.grid_width)\n",
    "            y = random.randint(1, self.grid_width)\n",
    "            point= (x,y)\n",
    "            iiot_pos[i] =point   # list of iiot_positions \n",
    "            \n",
    "        return iiot_pos \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def state(self):   \n",
    "        \n",
    "        uav_pos = [self.UAVs_pos[i] for i in range(1, self.num_uav+1)]\n",
    "        l = []\n",
    "        for i in uav_pos:\n",
    "            \n",
    "            l.append(i[0])\n",
    "            l.append(i[1])\n",
    "            \n",
    "        all_states = []\n",
    "        for i in range(self.num_agents):\n",
    "\n",
    "            state = [self.task_size[i], self.cpu_cycle[i], self.tolerant_delay[i], self.iiot_pos[i][0], self.iiot_pos[i][1]]\n",
    "            state.extend(l)\n",
    "            state.append(args['cloud_channel_gain'])\n",
    "            h_channel_condition = [self.Maths.uav_channel_gain(j, self.iiot_pos[i]) for j in uav_pos]\n",
    "            state.extend(h_channel_condition)\n",
    "            \n",
    "            all_states.append(state)\n",
    "        \n",
    "        return  all_states    \n",
    "         \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def next_state(self):\n",
    "        \n",
    "        task_size, cpu_cycle, tolerant_delay = self.task_model()\n",
    "        uav_pos = [self.UAVs_pos[i] for i in range(1, self.num_uav+1)]\n",
    "        iiot_pos = self.IIots_Position()\n",
    "        \n",
    "        l = []  \n",
    "        for i in range(self.num_uav):\n",
    "            \n",
    "            if uav_pos[i][0] + 30 <= self.grid_width:            # since uavs are moving with some speed in fixed area so taking it as 30.\n",
    "                \n",
    "                l.append(uav_pos[i][0]+30)\n",
    "                uav_pos[i][0] = uav_pos[i][0]+30\n",
    "                \n",
    "            else: \n",
    "                \n",
    "                l.append(uav_pos[i][0])\n",
    "               \n",
    "            \n",
    "            if uav_pos[i][1] + 30 <= self.grid_width:\n",
    "                \n",
    "                l.append(uav_pos[i][1] + 30)\n",
    "                uav_pos[i][1] = uav_pos[i][1]+30\n",
    "                \n",
    "            else: \n",
    "                \n",
    "                l.append( uav_pos[i][1] )\n",
    "\n",
    "            \n",
    "        all_next_states = []\n",
    "        for i in range(self.num_agents):\n",
    "\n",
    "            next_state = [task_size[i], cpu_cycle[i], tolerant_delay[i], iiot_pos[i][0], iiot_pos[i][1]]\n",
    "            next_state.extend(l)\n",
    "            next_state.append(args['cloud_channel_gain'])\n",
    "            h_channel_condition = [self.Maths.uav_channel_gain(j, iiot_pos[i]) for j in uav_pos]\n",
    "            next_state.extend(h_channel_condition)\n",
    "            \n",
    "            all_next_states.append(next_state)\n",
    "        \n",
    "        return  all_next_states \n",
    "         \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    def reward_calculate(self, execution_time, energy_consumption, tolerant_delay):\n",
    "        \n",
    "        if execution_time <= tolerant_delay: \n",
    "            \n",
    "            return 1/energy_consumption,  energy_consumption\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return (1/energy_consumption)* args['punishment_factor'] , energy_consumption\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def reward(self, agent_action, uav_pos, iiot_pos,task_size, cpu_cycle, tolerant_delay): \n",
    "        \n",
    "        if agent_action == -1:   # offload to cloud\n",
    "            \n",
    "            execution_time = self.Maths.cloud_transmission_time(task_size)\n",
    "            energy_consumption = self.Maths.cloud_energy_consumption(task_size)\n",
    "            reward, energy_consumption = self.reward_calculate(execution_time, energy_consumption, tolerant_delay)\n",
    "            \n",
    "            return reward, energy_consumption\n",
    "        \n",
    "        elif agent_action == 0:  # compute locally\n",
    "            \n",
    "            execution_time = self.Maths.local_computation_time(cpu_cycle)\n",
    "            energy_consumption = self.Maths.local_energy_consumption(cpu_cycle)\n",
    "            reward, energy_consumption= self.reward_calculate(execution_time, energy_consumption, tolerant_delay)\n",
    "            \n",
    "            return reward, energy_consumption\n",
    "        \n",
    "        else: # offload to UAV\n",
    "            \n",
    "            execution_time = self.Maths.uav_computation_time(uav_pos, iiot_pos, task_size, cpu_cycle)\n",
    "            energy_consumption = self.Maths.uav_energy_consumption(uav_pos, iiot_pos, task_size, cpu_cycle)\n",
    "            reward, energy_consumption= self.reward_calculate(execution_time, energy_consumption, tolerant_delay)\n",
    "            \n",
    "            return reward, energy_consumption\n",
    "    \n",
    "    \n",
    "    def done(self, actions):\n",
    "        \n",
    "        done = []\n",
    "        for i in range(len(actions)):\n",
    "            \n",
    "            if actions[i]> 0 :  #uav task \n",
    "                \n",
    "                a, b = self.UAVs_pos[actions[i]], self.iiot_pos[i]\n",
    "                distance = ((a[0]-b[0])**2 + (a[1]-b[1])**2)**(1/2)\n",
    "                execution_time = self.Maths.uav_computation_time( self.UAVs_pos[actions[i]], self.iiot_pos[i], self.task_size[i], self.cpu_cycle[i])\n",
    "                \n",
    "                if (execution_time <= self.tolerant_delay[i]) and (distance <= args['uav_range']):\n",
    "                    \n",
    "                    done.append(False)\n",
    "                    \n",
    "                else :\n",
    "                    \n",
    "                    done.append(True)\n",
    "                    \n",
    "            \n",
    "            elif actions[i] == 0:  # compute_locally\n",
    "                \n",
    "                execution_time = self.Maths.local_energy_consumption(self.cpu_cycle[i])\n",
    "                \n",
    "                if execution_time <= self.tolerant_delay[i] :\n",
    "                    \n",
    "                    done.append(False)\n",
    "                    \n",
    "                else: \n",
    "                    \n",
    "                    done.append(True)\n",
    "            \n",
    "            \n",
    "            else : # cloud task \n",
    "                \n",
    "                execution_time = self.Maths.cloud_transmission_time(self.task_size[i])\n",
    "                if execution_time <= self.tolerant_delay[i]:\n",
    "                    \n",
    "                    done.append(False)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    done.append(True)\n",
    "         \n",
    "        return done\n",
    "    \n",
    "                \n",
    "             \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, actions):  \n",
    "        \n",
    "        rewards = []\n",
    "        energy_consumption=[]\n",
    "        \n",
    "        for i,act in enumerate(actions): \n",
    "                \n",
    "                if act > 0:               # uav task\n",
    "                    \n",
    "                    r, e_c = self.reward(act, self.UAVs_pos[act], self.iiot_pos[i], self.task_size[i], self.cpu_cycle[i], self.tolerant_delay[i])\n",
    "                \n",
    "                elif act == 0:\n",
    "                    \n",
    "                    r, e_c= self.reward(act, [0,0,0],(0,0),self.task_size[i], self.cpu_cycle[i], self.tolerant_delay[i])\n",
    "                \n",
    "                else : \n",
    "                    \n",
    "                    r, e_c = self.reward(act, [0,0,0],(0,0),self.task_size[i], self.cpu_cycle[i], self.tolerant_delay[i])\n",
    "                \n",
    "                rewards.append(r)\n",
    "                energy_consumption.append(e_c)\n",
    "        \n",
    "        done = self.done(actions)   \n",
    "        next_state = self.next_state()    \n",
    "       \n",
    "        return next_state, rewards, energy_consumption, done, self.task_size, self.cpu_cycle, self.tolerant_delay\n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    def reset(self):\n",
    "        \n",
    "        self.UAV_Pos()\n",
    "        self.IIOT_Pos()\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be43557",
   "metadata": {
    "id": "2be43557"
   },
   "source": [
    "### ENV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aada9ea",
   "metadata": {
    "id": "6aada9ea"
   },
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self, args):\n",
    "        \n",
    "        self.step_b_update = args['step_b_update']\n",
    "    \n",
    "        \n",
    "        \n",
    "    def main(self):\n",
    "        \n",
    "        total_step = 0\n",
    "        for episode in range(1): \n",
    "            \n",
    "            print(\"mdspr\")\n",
    "            print(f\"--------------\\n1. Number of iiot devices : {args['agent_number']}\")\n",
    "            print(f\"2. Total Training steps : {args['max_timesteps']}\\n\\n\\n\")\n",
    "    \n",
    "           \n",
    "            time_step = 0\n",
    "            all_loss = []\n",
    "            all_done=[]\n",
    "            all_rewards = []\n",
    "            all_actions = []\n",
    "            all_energy_consumption=[]\n",
    "            all_task_size = []\n",
    "            all_cpu_cycle=[]\n",
    "            all_tolerant_delay=[]\n",
    "            \n",
    "            while time_step < args['max_timesteps']:\n",
    "                \n",
    "                env = Environment(args)\n",
    "                if total_step==0:\n",
    "                    state = env.state()\n",
    "                    agent = Agent(env.state_size, env.action_size, args)\n",
    "                    \n",
    "                print(\"Training Step : \", time_step,\"\\n\")\n",
    "                \n",
    "                actions = []   \n",
    "                for i in range(args['agent_number']):\n",
    "                    \n",
    "                    actions.append(agent.choose_action(tf.convert_to_tensor(state[i]))) \n",
    "                \n",
    "                next_state, reward,energy_consumption, done,  task_size, cpu_cycle, tolerant_delay = env.step(actions)\n",
    "                \n",
    "                next_state_copy = next_state\n",
    "                \n",
    "                state = np.reshape(state, (args['agent_number'], args['uav_number']*3 +6))\n",
    "                state = tf.convert_to_tensor(state)\n",
    "                \n",
    "                next_state = np.reshape(next_state, (args['agent_number'], args['uav_number']*3 +6))\n",
    "                next_state = tf.convert_to_tensor(next_state)\n",
    "                \n",
    "                batch = []\n",
    "                if args['memory_type']==\"per\":  # prioritised experience replay memory \n",
    "                    \n",
    "                    for i in range(args['agent_number']):\n",
    "\n",
    "                        batch.append((0, (state[i], actions[i], reward[i], next_state[i], done[i])))\n",
    "\n",
    "                    loss = agent.per_observe(batch)   \n",
    "\n",
    "                    if (total_step % args['step_b_update'] == 0) and (total_step!=0):\n",
    "\n",
    "                        agent.per_replay()\n",
    "                        \n",
    "                \n",
    "                else :   # uniform experience replay\n",
    "                    \n",
    "                    for i in range(args['agent_number']):\n",
    "        \n",
    "                        batch.append((state[i], actions[i], reward[i], next_state[i], done[i]))\n",
    "                \n",
    "                    loss = agent.uer_observe(batch)   \n",
    "                \n",
    "                    if (total_step % args['step_b_update'] == 0) and (total_step!=0):\n",
    "            \n",
    "                        agent.uer_replay()\n",
    "                    \n",
    "\n",
    "\n",
    "                        \n",
    "                if (total_step % args['target_frequency'])  and (total_step!=0)  == 0:\n",
    "                    \n",
    "                    agent.update_target_model()\n",
    "                    \n",
    "          #-------------------------------\n",
    "                loss_list=[]\n",
    "                for i in range(args['agent_number']):\n",
    "                    loss_list.append(loss[i])\n",
    "                \n",
    "                all_task_size.append(task_size)\n",
    "                all_cpu_cycle.append(cpu_cycle)\n",
    "                all_tolerant_delay.append(tolerant_delay)\n",
    "                all_energy_consumption.append(energy_consumption)\n",
    "                all_actions.append(actions)\n",
    "                all_rewards.append(reward)\n",
    "                all_done.append(done)\n",
    "                all_loss.append(loss_list)       \n",
    "         #---------------------------------            \n",
    "                \n",
    "                time_step +=1\n",
    "                total_step +=1\n",
    "                state = next_state_copy\n",
    "                \n",
    "            print(\"\\n\",\"*\"*110,\"\\n\"*5)\n",
    "            \n",
    "            \n",
    "            return all_loss, all_energy_consumption, all_done, all_rewards, all_actions, all_task_size, all_cpu_cycle, all_tolerant_delay\n",
    "        \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cafca931",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cafca931",
    "outputId": "01b54e2f-fa63-4c88-afff-b6384e2aad44",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdspr\n",
      "--------------\n",
      "1. Number of iiot devices : 50\n",
      "2. Total Training steps : 500\n",
      "\n",
      "\n",
      "\n",
      "Training Step :  0 \n",
      "\n",
      "Training Step :  1 \n",
      "\n",
      "Training Step :  2 \n",
      "\n",
      "Training Step :  3 \n",
      "\n",
      "Training Step :  4 \n",
      "\n",
      "Training Step :  5 \n",
      "\n",
      "Training Step :  6 \n",
      "\n",
      "Training Step :  7 \n",
      "\n",
      "Training Step :  8 \n",
      "\n",
      "Training Step :  9 \n",
      "\n",
      "Training Step :  10 \n",
      "\n",
      "Training Step :  11 \n",
      "\n",
      "Training Step :  12 \n",
      "\n",
      "Training Step :  13 \n",
      "\n",
      "Training Step :  14 \n",
      "\n",
      "Training Step :  15 \n",
      "\n",
      "Training Step :  16 \n",
      "\n",
      "Training Step :  17 \n",
      "\n",
      "Training Step :  18 \n",
      "\n",
      "Training Step :  19 \n",
      "\n",
      "Training Step :  20 \n",
      "\n",
      "Training Step :  21 \n",
      "\n",
      "Training Step :  22 \n",
      "\n",
      "Training Step :  23 \n",
      "\n",
      "Training Step :  24 \n",
      "\n",
      "Training Step :  25 \n",
      "\n",
      "Training Step :  26 \n",
      "\n",
      "Training Step :  27 \n",
      "\n",
      "Training Step :  28 \n",
      "\n",
      "Training Step :  29 \n",
      "\n",
      "Training Step :  30 \n",
      "\n",
      "Training Step :  31 \n",
      "\n",
      "Training Step :  32 \n",
      "\n",
      "Training Step :  33 \n",
      "\n",
      "Training Step :  34 \n",
      "\n",
      "Training Step :  35 \n",
      "\n",
      "Training Step :  36 \n",
      "\n",
      "Training Step :  37 \n",
      "\n",
      "Training Step :  38 \n",
      "\n",
      "Training Step :  39 \n",
      "\n",
      "Training Step :  40 \n",
      "\n",
      "Training Step :  41 \n",
      "\n",
      "Training Step :  42 \n",
      "\n",
      "Training Step :  43 \n",
      "\n",
      "Training Step :  44 \n",
      "\n",
      "Training Step :  45 \n",
      "\n",
      "Training Step :  46 \n",
      "\n",
      "Training Step :  47 \n",
      "\n",
      "Training Step :  48 \n",
      "\n",
      "Training Step :  49 \n",
      "\n",
      "Training Step :  50 \n",
      "\n",
      "Training Step :  51 \n",
      "\n",
      "Training Step :  52 \n",
      "\n",
      "Training Step :  53 \n",
      "\n",
      "Training Step :  54 \n",
      "\n",
      "Training Step :  55 \n",
      "\n",
      "Training Step :  56 \n",
      "\n",
      "Training Step :  57 \n",
      "\n",
      "Training Step :  58 \n",
      "\n",
      "Training Step :  59 \n",
      "\n",
      "Training Step :  60 \n",
      "\n",
      "Training Step :  61 \n",
      "\n",
      "Training Step :  62 \n",
      "\n",
      "Training Step :  63 \n",
      "\n",
      "Training Step :  64 \n",
      "\n",
      "Training Step :  65 \n",
      "\n",
      "Training Step :  66 \n",
      "\n",
      "Training Step :  67 \n",
      "\n",
      "Training Step :  68 \n",
      "\n",
      "Training Step :  69 \n",
      "\n",
      "Training Step :  70 \n",
      "\n",
      "Training Step :  71 \n",
      "\n",
      "Training Step :  72 \n",
      "\n",
      "Training Step :  73 \n",
      "\n",
      "Training Step :  74 \n",
      "\n",
      "Training Step :  75 \n",
      "\n",
      "Training Step :  76 \n",
      "\n",
      "Training Step :  77 \n",
      "\n",
      "Training Step :  78 \n",
      "\n",
      "Training Step :  79 \n",
      "\n",
      "Training Step :  80 \n",
      "\n",
      "Training Step :  81 \n",
      "\n",
      "Training Step :  82 \n",
      "\n",
      "Training Step :  83 \n",
      "\n",
      "Training Step :  84 \n",
      "\n",
      "Training Step :  85 \n",
      "\n",
      "Training Step :  86 \n",
      "\n",
      "Training Step :  87 \n",
      "\n",
      "Training Step :  88 \n",
      "\n",
      "Training Step :  89 \n",
      "\n",
      "Training Step :  90 \n",
      "\n",
      "Training Step :  91 \n",
      "\n",
      "Training Step :  92 \n",
      "\n",
      "Training Step :  93 \n",
      "\n",
      "Training Step :  94 \n",
      "\n",
      "Training Step :  95 \n",
      "\n",
      "Training Step :  96 \n",
      "\n",
      "Training Step :  97 \n",
      "\n",
      "Training Step :  98 \n",
      "\n",
      "Training Step :  99 \n",
      "\n",
      "Training Step :  100 \n",
      "\n",
      "Training Step :  101 \n",
      "\n",
      "Training Step :  102 \n",
      "\n",
      "Training Step :  103 \n",
      "\n",
      "Training Step :  104 \n",
      "\n",
      "Training Step :  105 \n",
      "\n",
      "Training Step :  106 \n",
      "\n",
      "Training Step :  107 \n",
      "\n",
      "Training Step :  108 \n",
      "\n",
      "Training Step :  109 \n",
      "\n",
      "Training Step :  110 \n",
      "\n",
      "Training Step :  111 \n",
      "\n",
      "Training Step :  112 \n",
      "\n",
      "Training Step :  113 \n",
      "\n",
      "Training Step :  114 \n",
      "\n",
      "Training Step :  115 \n",
      "\n",
      "Training Step :  116 \n",
      "\n",
      "Training Step :  117 \n",
      "\n",
      "Training Step :  118 \n",
      "\n",
      "Training Step :  119 \n",
      "\n",
      "Training Step :  120 \n",
      "\n",
      "Training Step :  121 \n",
      "\n",
      "Training Step :  122 \n",
      "\n",
      "Training Step :  123 \n",
      "\n",
      "Training Step :  124 \n",
      "\n",
      "Training Step :  125 \n",
      "\n",
      "Training Step :  126 \n",
      "\n",
      "Training Step :  127 \n",
      "\n",
      "Training Step :  128 \n",
      "\n",
      "Training Step :  129 \n",
      "\n",
      "Training Step :  130 \n",
      "\n",
      "Training Step :  131 \n",
      "\n",
      "Training Step :  132 \n",
      "\n",
      "Training Step :  133 \n",
      "\n",
      "Training Step :  134 \n",
      "\n",
      "Training Step :  135 \n",
      "\n",
      "Training Step :  136 \n",
      "\n",
      "Training Step :  137 \n",
      "\n",
      "Training Step :  138 \n",
      "\n",
      "Training Step :  139 \n",
      "\n",
      "Training Step :  140 \n",
      "\n",
      "Training Step :  141 \n",
      "\n",
      "Training Step :  142 \n",
      "\n",
      "Training Step :  143 \n",
      "\n",
      "Training Step :  144 \n",
      "\n",
      "Training Step :  145 \n",
      "\n",
      "Training Step :  146 \n",
      "\n",
      "Training Step :  147 \n",
      "\n",
      "Training Step :  148 \n",
      "\n",
      "Training Step :  149 \n",
      "\n",
      "Training Step :  150 \n",
      "\n",
      "Training Step :  151 \n",
      "\n",
      "Training Step :  152 \n",
      "\n",
      "Training Step :  153 \n",
      "\n",
      "Training Step :  154 \n",
      "\n",
      "Training Step :  155 \n",
      "\n",
      "Training Step :  156 \n",
      "\n",
      "Training Step :  157 \n",
      "\n",
      "Training Step :  158 \n",
      "\n",
      "Training Step :  159 \n",
      "\n",
      "Training Step :  160 \n",
      "\n",
      "Training Step :  161 \n",
      "\n",
      "Training Step :  162 \n",
      "\n",
      "Training Step :  163 \n",
      "\n",
      "Training Step :  164 \n",
      "\n",
      "Training Step :  165 \n",
      "\n",
      "Training Step :  166 \n",
      "\n",
      "Training Step :  167 \n",
      "\n",
      "Training Step :  168 \n",
      "\n",
      "Training Step :  169 \n",
      "\n",
      "Training Step :  170 \n",
      "\n",
      "Training Step :  171 \n",
      "\n",
      "Training Step :  172 \n",
      "\n",
      "Training Step :  173 \n",
      "\n",
      "Training Step :  174 \n",
      "\n",
      "Training Step :  175 \n",
      "\n",
      "Training Step :  176 \n",
      "\n",
      "Training Step :  177 \n",
      "\n",
      "Training Step :  178 \n",
      "\n",
      "Training Step :  179 \n",
      "\n",
      "Training Step :  180 \n",
      "\n",
      "Training Step :  181 \n",
      "\n",
      "Training Step :  182 \n",
      "\n",
      "Training Step :  183 \n",
      "\n",
      "Training Step :  184 \n",
      "\n",
      "Training Step :  185 \n",
      "\n",
      "Training Step :  186 \n",
      "\n",
      "Training Step :  187 \n",
      "\n",
      "Training Step :  188 \n",
      "\n",
      "Training Step :  189 \n",
      "\n",
      "Training Step :  190 \n",
      "\n",
      "Training Step :  191 \n",
      "\n",
      "Training Step :  192 \n",
      "\n",
      "Training Step :  193 \n",
      "\n",
      "Training Step :  194 \n",
      "\n",
      "Training Step :  195 \n",
      "\n",
      "Training Step :  196 \n",
      "\n",
      "Training Step :  197 \n",
      "\n",
      "Training Step :  198 \n",
      "\n",
      "Training Step :  199 \n",
      "\n",
      "Training Step :  200 \n",
      "\n",
      "# replay-------- \n",
      "Training Step :  201 \n",
      "\n",
      "Training Step :  202 \n",
      "\n",
      "Training Step :  203 \n",
      "\n",
      "Training Step :  204 \n",
      "\n",
      "Training Step :  205 \n",
      "\n",
      "Training Step :  206 \n",
      "\n",
      "Training Step :  207 \n",
      "\n",
      "Training Step :  208 \n",
      "\n",
      "Training Step :  209 \n",
      "\n",
      "Training Step :  210 \n",
      "\n",
      "Training Step :  211 \n",
      "\n",
      "Training Step :  212 \n",
      "\n",
      "Training Step :  213 \n",
      "\n",
      "Training Step :  214 \n",
      "\n",
      "Training Step :  215 \n",
      "\n",
      "Training Step :  216 \n",
      "\n",
      "Training Step :  217 \n",
      "\n",
      "Training Step :  218 \n",
      "\n",
      "Training Step :  219 \n",
      "\n",
      "Training Step :  220 \n",
      "\n",
      "Training Step :  221 \n",
      "\n",
      "Training Step :  222 \n",
      "\n",
      "Training Step :  223 \n",
      "\n",
      "Training Step :  224 \n",
      "\n",
      "Training Step :  225 \n",
      "\n",
      "Training Step :  226 \n",
      "\n",
      "Training Step :  227 \n",
      "\n",
      "Training Step :  228 \n",
      "\n",
      "Training Step :  229 \n",
      "\n",
      "Training Step :  230 \n",
      "\n",
      "Training Step :  231 \n",
      "\n",
      "Training Step :  232 \n",
      "\n",
      "Training Step :  233 \n",
      "\n",
      "Training Step :  234 \n",
      "\n",
      "Training Step :  235 \n",
      "\n",
      "Training Step :  236 \n",
      "\n",
      "Training Step :  237 \n",
      "\n",
      "Training Step :  238 \n",
      "\n",
      "Training Step :  239 \n",
      "\n",
      "Training Step :  240 \n",
      "\n",
      "Training Step :  241 \n",
      "\n",
      "Training Step :  242 \n",
      "\n",
      "Training Step :  243 \n",
      "\n",
      "Training Step :  244 \n",
      "\n",
      "Training Step :  245 \n",
      "\n",
      "Training Step :  246 \n",
      "\n",
      "Training Step :  247 \n",
      "\n",
      "Training Step :  248 \n",
      "\n",
      "Training Step :  249 \n",
      "\n",
      "Training Step :  250 \n",
      "\n",
      "Training Step :  251 \n",
      "\n",
      "Training Step :  252 \n",
      "\n",
      "Training Step :  253 \n",
      "\n",
      "Training Step :  254 \n",
      "\n",
      "Training Step :  255 \n",
      "\n",
      "Training Step :  256 \n",
      "\n",
      "Training Step :  257 \n",
      "\n",
      "Training Step :  258 \n",
      "\n",
      "Training Step :  259 \n",
      "\n",
      "Training Step :  260 \n",
      "\n",
      "Training Step :  261 \n",
      "\n",
      "Training Step :  262 \n",
      "\n",
      "Training Step :  263 \n",
      "\n",
      "Training Step :  264 \n",
      "\n",
      "Training Step :  265 \n",
      "\n",
      "Training Step :  266 \n",
      "\n",
      "Training Step :  267 \n",
      "\n",
      "Training Step :  268 \n",
      "\n",
      "Training Step :  269 \n",
      "\n",
      "Training Step :  270 \n",
      "\n",
      "Training Step :  271 \n",
      "\n",
      "Training Step :  272 \n",
      "\n",
      "Training Step :  273 \n",
      "\n",
      "Training Step :  274 \n",
      "\n",
      "Training Step :  275 \n",
      "\n",
      "Training Step :  276 \n",
      "\n",
      "Training Step :  277 \n",
      "\n",
      "Training Step :  278 \n",
      "\n",
      "Training Step :  279 \n",
      "\n",
      "Training Step :  280 \n",
      "\n",
      "Training Step :  281 \n",
      "\n",
      "Training Step :  282 \n",
      "\n",
      "Training Step :  283 \n",
      "\n",
      "Training Step :  284 \n",
      "\n",
      "Training Step :  285 \n",
      "\n",
      "Training Step :  286 \n",
      "\n",
      "Training Step :  287 \n",
      "\n",
      "Training Step :  288 \n",
      "\n",
      "Training Step :  289 \n",
      "\n",
      "Training Step :  290 \n",
      "\n",
      "Training Step :  291 \n",
      "\n",
      "Training Step :  292 \n",
      "\n",
      "Training Step :  293 \n",
      "\n",
      "Training Step :  294 \n",
      "\n",
      "Training Step :  295 \n",
      "\n",
      "Training Step :  296 \n",
      "\n",
      "Training Step :  297 \n",
      "\n",
      "Training Step :  298 \n",
      "\n",
      "Training Step :  299 \n",
      "\n",
      "Training Step :  300 \n",
      "\n",
      "Training Step :  301 \n",
      "\n",
      "Training Step :  302 \n",
      "\n",
      "Training Step :  303 \n",
      "\n",
      "Training Step :  304 \n",
      "\n",
      "Training Step :  305 \n",
      "\n",
      "Training Step :  306 \n",
      "\n",
      "Training Step :  307 \n",
      "\n",
      "Training Step :  308 \n",
      "\n",
      "Training Step :  309 \n",
      "\n",
      "Training Step :  310 \n",
      "\n",
      "Training Step :  311 \n",
      "\n",
      "Training Step :  312 \n",
      "\n",
      "Training Step :  313 \n",
      "\n",
      "Training Step :  314 \n",
      "\n",
      "Training Step :  315 \n",
      "\n",
      "Training Step :  316 \n",
      "\n",
      "Training Step :  317 \n",
      "\n",
      "Training Step :  318 \n",
      "\n",
      "Training Step :  319 \n",
      "\n",
      "Training Step :  320 \n",
      "\n",
      "Training Step :  321 \n",
      "\n",
      "Training Step :  322 \n",
      "\n",
      "Training Step :  323 \n",
      "\n",
      "Training Step :  324 \n",
      "\n",
      "Training Step :  325 \n",
      "\n",
      "Training Step :  326 \n",
      "\n",
      "Training Step :  327 \n",
      "\n",
      "Training Step :  328 \n",
      "\n",
      "Training Step :  329 \n",
      "\n",
      "Training Step :  330 \n",
      "\n",
      "Training Step :  331 \n",
      "\n",
      "Training Step :  332 \n",
      "\n",
      "Training Step :  333 \n",
      "\n",
      "Training Step :  334 \n",
      "\n",
      "Training Step :  335 \n",
      "\n",
      "Training Step :  336 \n",
      "\n",
      "Training Step :  337 \n",
      "\n",
      "Training Step :  338 \n",
      "\n",
      "Training Step :  339 \n",
      "\n",
      "Training Step :  340 \n",
      "\n",
      "Training Step :  341 \n",
      "\n",
      "Training Step :  342 \n",
      "\n",
      "Training Step :  343 \n",
      "\n",
      "Training Step :  344 \n",
      "\n",
      "Training Step :  345 \n",
      "\n",
      "Training Step :  346 \n",
      "\n",
      "Training Step :  347 \n",
      "\n",
      "Training Step :  348 \n",
      "\n",
      "Training Step :  349 \n",
      "\n",
      "Training Step :  350 \n",
      "\n",
      "Training Step :  351 \n",
      "\n",
      "Training Step :  352 \n",
      "\n",
      "Training Step :  353 \n",
      "\n",
      "Training Step :  354 \n",
      "\n",
      "Training Step :  355 \n",
      "\n",
      "Training Step :  356 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step :  357 \n",
      "\n",
      "Training Step :  358 \n",
      "\n",
      "Training Step :  359 \n",
      "\n",
      "Training Step :  360 \n",
      "\n",
      "Training Step :  361 \n",
      "\n",
      "Training Step :  362 \n",
      "\n",
      "Training Step :  363 \n",
      "\n",
      "Training Step :  364 \n",
      "\n",
      "Training Step :  365 \n",
      "\n",
      "Training Step :  366 \n",
      "\n",
      "Training Step :  367 \n",
      "\n",
      "Training Step :  368 \n",
      "\n",
      "Training Step :  369 \n",
      "\n",
      "Training Step :  370 \n",
      "\n",
      "Training Step :  371 \n",
      "\n",
      "Training Step :  372 \n",
      "\n",
      "Training Step :  373 \n",
      "\n",
      "Training Step :  374 \n",
      "\n",
      "Training Step :  375 \n",
      "\n",
      "Training Step :  376 \n",
      "\n",
      "Training Step :  377 \n",
      "\n",
      "Training Step :  378 \n",
      "\n",
      "Training Step :  379 \n",
      "\n",
      "Training Step :  380 \n",
      "\n",
      "Training Step :  381 \n",
      "\n",
      "Training Step :  382 \n",
      "\n",
      "Training Step :  383 \n",
      "\n",
      "Training Step :  384 \n",
      "\n",
      "Training Step :  385 \n",
      "\n",
      "Training Step :  386 \n",
      "\n",
      "Training Step :  387 \n",
      "\n",
      "Training Step :  388 \n",
      "\n",
      "Training Step :  389 \n",
      "\n",
      "Training Step :  390 \n",
      "\n",
      "Training Step :  391 \n",
      "\n",
      "Training Step :  392 \n",
      "\n",
      "Training Step :  393 \n",
      "\n",
      "Training Step :  394 \n",
      "\n",
      "Training Step :  395 \n",
      "\n",
      "Training Step :  396 \n",
      "\n",
      "Training Step :  397 \n",
      "\n",
      "Training Step :  398 \n",
      "\n",
      "Training Step :  399 \n",
      "\n",
      "Training Step :  400 \n",
      "\n",
      "# replay-------- \n",
      "Training Step :  401 \n",
      "\n",
      "Training Step :  402 \n",
      "\n",
      "Training Step :  403 \n",
      "\n",
      "Training Step :  404 \n",
      "\n",
      "Training Step :  405 \n",
      "\n",
      "Training Step :  406 \n",
      "\n",
      "Training Step :  407 \n",
      "\n",
      "Training Step :  408 \n",
      "\n",
      "Training Step :  409 \n",
      "\n",
      "Training Step :  410 \n",
      "\n",
      "Training Step :  411 \n",
      "\n",
      "Training Step :  412 \n",
      "\n",
      "Training Step :  413 \n",
      "\n",
      "Training Step :  414 \n",
      "\n",
      "Training Step :  415 \n",
      "\n",
      "Training Step :  416 \n",
      "\n",
      "Training Step :  417 \n",
      "\n",
      "Training Step :  418 \n",
      "\n",
      "Training Step :  419 \n",
      "\n",
      "Training Step :  420 \n",
      "\n",
      "Training Step :  421 \n",
      "\n",
      "Training Step :  422 \n",
      "\n",
      "Training Step :  423 \n",
      "\n",
      "Training Step :  424 \n",
      "\n",
      "Training Step :  425 \n",
      "\n",
      "Training Step :  426 \n",
      "\n",
      "Training Step :  427 \n",
      "\n",
      "Training Step :  428 \n",
      "\n",
      "Training Step :  429 \n",
      "\n",
      "Training Step :  430 \n",
      "\n",
      "Training Step :  431 \n",
      "\n",
      "Training Step :  432 \n",
      "\n",
      "Training Step :  433 \n",
      "\n",
      "Training Step :  434 \n",
      "\n",
      "Training Step :  435 \n",
      "\n",
      "Training Step :  436 \n",
      "\n",
      "Training Step :  437 \n",
      "\n",
      "Training Step :  438 \n",
      "\n",
      "Training Step :  439 \n",
      "\n",
      "Training Step :  440 \n",
      "\n",
      "Training Step :  441 \n",
      "\n",
      "Training Step :  442 \n",
      "\n",
      "Training Step :  443 \n",
      "\n",
      "Training Step :  444 \n",
      "\n",
      "Training Step :  445 \n",
      "\n",
      "Training Step :  446 \n",
      "\n",
      "Training Step :  447 \n",
      "\n",
      "Training Step :  448 \n",
      "\n",
      "Training Step :  449 \n",
      "\n",
      "Training Step :  450 \n",
      "\n",
      "Training Step :  451 \n",
      "\n",
      "Training Step :  452 \n",
      "\n",
      "Training Step :  453 \n",
      "\n",
      "Training Step :  454 \n",
      "\n",
      "Training Step :  455 \n",
      "\n",
      "Training Step :  456 \n",
      "\n",
      "Training Step :  457 \n",
      "\n",
      "Training Step :  458 \n",
      "\n",
      "Training Step :  459 \n",
      "\n",
      "Training Step :  460 \n",
      "\n",
      "Training Step :  461 \n",
      "\n",
      "Training Step :  462 \n",
      "\n",
      "Training Step :  463 \n",
      "\n",
      "Training Step :  464 \n",
      "\n",
      "Training Step :  465 \n",
      "\n",
      "Training Step :  466 \n",
      "\n",
      "Training Step :  467 \n",
      "\n",
      "Training Step :  468 \n",
      "\n",
      "Training Step :  469 \n",
      "\n",
      "Training Step :  470 \n",
      "\n",
      "Training Step :  471 \n",
      "\n",
      "Training Step :  472 \n",
      "\n",
      "Training Step :  473 \n",
      "\n",
      "Training Step :  474 \n",
      "\n",
      "Training Step :  475 \n",
      "\n",
      "Training Step :  476 \n",
      "\n",
      "Training Step :  477 \n",
      "\n",
      "Training Step :  478 \n",
      "\n",
      "Training Step :  479 \n",
      "\n",
      "Training Step :  480 \n",
      "\n",
      "Training Step :  481 \n",
      "\n",
      "Training Step :  482 \n",
      "\n",
      "Training Step :  483 \n",
      "\n",
      "Training Step :  484 \n",
      "\n",
      "Training Step :  485 \n",
      "\n",
      "Training Step :  486 \n",
      "\n",
      "Training Step :  487 \n",
      "\n",
      "Training Step :  488 \n",
      "\n",
      "Training Step :  489 \n",
      "\n",
      "Training Step :  490 \n",
      "\n",
      "Training Step :  491 \n",
      "\n",
      "Training Step :  492 \n",
      "\n",
      "Training Step :  493 \n",
      "\n",
      "Training Step :  494 \n",
      "\n",
      "Training Step :  495 \n",
      "\n",
      "Training Step :  496 \n",
      "\n",
      "Training Step :  497 \n",
      "\n",
      "Training Step :  498 \n",
      "\n",
      "Training Step :  499 \n",
      "\n",
      "\n",
      " ************************************************************************************************************** \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    for i in [30]:\n",
    "        parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "        parser.add_argument(\"-f\")\n",
    "        parser.add_argument(\"-lr\", \"--learning_rate\", default=0.0001, type=float, help=\"learning rate\")\n",
    "        parser.add_argument(\"-tf\", \"--target_frequency\", default=400, type=int, help=\"target weights replace steps\") # in paper 500\n",
    "        parser.add_argument(\"-bs\", \"--batch_size\", default=50, type=int, help=\"batch size\")\n",
    "        parser.add_argument(\"-ga\", \"--gamma\", default=0.7, type=float, help=\"reward decay rate\")\n",
    "        parser.add_argument(\"-e\", \"--epsilon\", default=0.9, type=float, help=\"exploration rate\")\n",
    "        parser.add_argument(\"-c\", \"--memory_capacity\", default=4000, type=int, help=\"replay memory capacity\")\n",
    "        parser.add_argument(\"-nn\", \"--number_nodes\", default=100, type=int, help=\"number of nodes in each layer of neural network\")\n",
    "        parser.add_argument(\"-m\", \"--agent_number\", default=i, type=int, help=\"total number of iiot devices\")  # 1000 used in paper \n",
    "        parser.add_argument(\"-uav\", \"--uav_number\", default=5, type=int, help=\"total number of UAVs\")\n",
    "        parser.add_argument(\"-g\", \"--grid_width\", default=800, type=int, help=\"size of fixed area under consideration\")\n",
    "        parser.add_argument(\"-H\", \"--uav_height\" , default=100, type=int, help=\"flying height of UAV  'in meters'\")\n",
    "        parser.add_argument(\"-r\", \"--uav_range\", default=300, type=int, help=\"communication range of UAV 'in meters'\")\n",
    "        parser.add_argument(\"-cl\", \"--local_compute\", default=500e+6, type=float, help=\"local computation capacity 'cycle/second'\")  # 500MHz\n",
    "        parser.add_argument(\"-cu\", \"--uav_compute\", default=2e+9, type=float, help=\"UAV compution capacity 'cycle/second'\")  # 2GHz\n",
    "        parser.add_argument(\"-cc\", \"--cloud_compute\", default=100e+9, type=float, help=\"cloud computation capacity 'cycle/second'\")  #  100GHz\n",
    "        parser.add_argument(\"-rd\", \"--reference_distance\", default=1, type=float, help=\"channel gain reference distance 'meters'\")\n",
    "        parser.add_argument(\"-lcp\", \"--los_channel_power\", default=1.42e-4, type=float, help=\"channel gain at the reference\")\n",
    "        parser.add_argument(\"-ub\", \"--uav_bandwidth\", default=15e+6, type=float, help=\"bandwidth allocated for UAV uplin transmission rate 'cycles/second'\")  # 15MHz\n",
    "        parser.add_argument(\"-cb\", \"--cloud_bandwidth\", default=10e+6, type=float, help=\"bandwidth allocated for cloud uplink transmission 'cycles/second'\")  #  10MHz\n",
    "        parser.add_argument(\"-up\",\"--uav_power\", default=0.01, type=float, help=\"uplink transmission power for UAV offloading  'W'\")\n",
    "        parser.add_argument(\"-cp\", \"--cloud_power\", default=0.015, type=float, help=\"uplink transmission power for cloud offloading  'W'\")\n",
    "        parser.add_argument(\"-n\", \"--noise_power\", default=1e-12, type=float, help=\"background noise power  'Watt-second'\") # -90 dBm/Hz\n",
    "        parser.add_argument(\"-ptf\", \"--propagation_time_factor\", default=3.2e-5, type=float, help=\"uplink propogation delay factor  's/Kb'\")  # 4e-9 s/bit\n",
    "        parser.add_argument(\"-lec\", \"--local_energy_consumption_factor\", default=1e-23, type=float, help=\"local energy consumption factor 'theta' J/cycle\")\n",
    "\n",
    "\n",
    "          # value not given in paper\n",
    "          #------------------------------------------------------------------------------------------------------------------------------------\n",
    "        parser.add_argument(\"-ccg\",\"--cloud_channel_gain\", default=280.141199827, type=float, help=\"cloud channel gain H(t)\")\n",
    "        parser.add_argument(\"-b_stp\", \"--step_b_update\",default=400, type=int, help=\"steps between updating the network\")\n",
    "        parser.add_argument(\"-pf\", \"--punishment_factor\", default=0.0001, type = float, help=\"if tolerant delay < energy consumption\")  \n",
    "        parser.add_argument(\"-p\", \"--priority_scale\", default=0.5, type=float, help=\"scale for prioritization\")  \n",
    "        parser.add_argument(\"-m_e\", \"--min_epsilon\", default=0.02, type=float, help=\"minimum value of exploration rate\")\n",
    "        parser.add_argument(\"-e_d\", \"--epsilon_decay\", default=1e-4, type=float, help=\"exploration decay rate\")\n",
    "        parser.add_argument('-m_b', \"--min_beta\", default=0.4, type=float, help=\"minimum value of importance sampling\")\n",
    "        parser.add_argument(\"-b_d\",\"--beta_increment\", default=1e-4, type=float, help=\"for incrementing beta value\")\n",
    "        parser.add_argument(\"-b_max\", \"--beta_max\", default=0.9, type=float, help=\"incrementing value of importance sampling beta\")\n",
    "        parser.add_argument(\"-ts\", \"--max_timesteps\", default=3500, type=int, help=\"maximum timesteps in each epsisode\")  ## value not given in paper\n",
    "        parser.add_argument(\"-ed\", \"--episodes\", default=1, type=int, help=\"total number of episodes\")    ## value not given in paper \n",
    "\n",
    "        parser.add_argument(\"-mt\", \"--memory_type\", choices=['uer', 'per'], default=\"per\", help=\"per: prioritised experience replay, uer: uniform experience replay\")\n",
    "        parser.add_argument('-ddqn', \"--double_dqn\", choices=['dqn',\"double_dqn\"], default=\"double_dqn\", help =\"double deep Q network or DQN \")\n",
    "        parser.add_argument(\"-dueling\",\"--dueling\", default=False, type=bool, help=\"Dueling option\")\n",
    "          #-------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # cloud_channel_gain = 148.1 + 40  log10 distance(km)   (taking distance as 20Km)  = 148.1 + 40* (log10 2000)\n",
    "\n",
    "\n",
    "        args = vars(parser.parse_args())\n",
    "\n",
    "        env = ENV(args)\n",
    "\n",
    "        all_loss, all_energy_consumption, all_done, all_reward, all_action, all_task_size, all_cpu_cycle, all_tolerant_delay = env.main()\n",
    "\n",
    "        df_loss = pd.DataFrame(all_loss)\n",
    "        df_energy_consumption = pd.DataFrame(all_energy_consumption) \n",
    "        df_done = pd.DataFrame(all_done)\n",
    "        df_reward = pd.DataFrame(all_reward)\n",
    "        df_action = pd.DataFrame(all_action)\n",
    "        df_task_size=pd.DataFrame(all_task_size)\n",
    "        df_cpu_cycle=pd.DataFrame(all_cpu_cycle)\n",
    "        df_tolerant_delay=pd.DataFrame(all_tolerant_delay)\n",
    "        \n",
    "\n",
    "#         df_loss.to_csv(\"datasets/fig_8/xyz_md2spr_df_loss.csv\")\n",
    "#         df_energy_consumption.to_csv(\"datasets/fig_8/xyz_md2spr_df_energy_consumption.csv\")\n",
    "#         df_done.to_csv(\"datasets/fig_8/xyz_md2spr_df_done.csv\")\n",
    "#         df_reward.to_csv(\"datasets/fig_8/xyz_md2spr_reward.csv\")\n",
    "#         df_action.to_csv(\"datasets/fig_8/xyz_md2spr_df_action.csv\")\n",
    "#         df_task_size.to_csv(\"datasets/fig_8/xyz_md2spr_df_task_size.csv\")\n",
    "#         df_cpu_cycle.to_csv(\"datasets/fig_8/xyz_md2spr_df_task_size.csv\")\n",
    "#         df_tolerant_delay.to_csv(\"datasets/fig_8/xyz_md2spr_df_task_size.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285fff3d",
   "metadata": {
    "id": "285fff3d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf357fd",
   "metadata": {
    "id": "bcf357fd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f04144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1. (MDSPR) Dqn with per code .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
